{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GW Libraries and Academic Innovation**\n",
    "\n",
    "Tuesday, October 27, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workshop goals\n",
    "\n",
    "This workshop introduces some fundamental concepts of natural-language processing through a hands-on approach with Python.\n",
    "\n",
    "By the conclusion of this workshop, you will have worked through the following:\n",
    "- Processing a collection of texts with the `spacy` Python library\n",
    "- Exploring core features of `spacy` for NLP, including tokenization, part-of-speech tagging, and named-entity recognition\n",
    "- Computing basic metrics about the dateset using some of these features\n",
    "- Exploring vector-based word-representation as a measure of document similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for using this Google Colab notebook\n",
    "\n",
    "When working in a Google Colaboratory notebook, `Shift-Return` (`Shift-Enter`) runs the cell you're on. You can also run the cell using the `Play` button at the left edge of the cell.\n",
    "\n",
    "There are many other keyboard shortcuts. You can access the list via the menu bar, at `Tools`-->`Command palette`. In fact, you can even customize your keyboard shortcuts using `Tools`-->`Keyboard shortcuts`.\n",
    "\n",
    "(If you're working in an Anaconda/Jupyter notebook: \n",
    " - `Control-Enter` (`Command-Return`) runs the cell you're on. You can also run the cell using the `Run` button in the toolbar. `Esc`, then `A` inserts a cell above where you are.\n",
    " - `Esc`, then `B` inserts a cell below where you are.\n",
    " - More shortcuts under `Help` --> `Keyboard Shortcuts`)\n",
    "\n",
    "You will probably get some errors in working through this notebook. That's okay, you can just go back and change the cell and re-run it.\n",
    "\n",
    "The notebook auto-saves as you work, just like gmail and most Google apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "#### Defining NLP\n",
    "\n",
    "For the purposes of this workshop, natural-language processing (NLP for short) refers to the use of computational methods to analyze samples of written or spoken language produced by human beings. The term _natural_ refers to the fact that the languages in question are those that have emerged from the crucible of human speech and social interaction. An example of a language **not** considered natural would be a programming language, such as Python. Computers, of course, regularly process these kinds of languages; in a sense, that is what they are designed to do. \n",
    "\n",
    "Natural language, for many reasons, some of which we will explore below, prove far less amenable to computational processing. \n",
    "\n",
    "#### Why NLP?\n",
    "\n",
    "- To make computers better at talking to us / thinking like us (semantic search, artificial intelligence)\n",
    "- To improve digital interfaces (voice recognition)\n",
    "- To automate human communicative tasks (chatbots, autocomplete, text generation)\n",
    "- To understand human discourse computationally (textual analysis)\n",
    "- To enhance surveillance, monitoring, and control (content filtering, predictive analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "[spaCy](https://spacy.io/) is a Python library for NLP. Other libraries exists, most notably [NLTK](https://www.nltk.org/) that provide similar functionality. But spaCy combines a user-friendly interface with high performance (the _Cy_ in _spaCy_ alludes to the fact that under the hood, a lot of the library is written in Cython, which is a hybrid of Python and the programming language C). \n",
    "\n",
    "If you haven't used it before, you'll probably need to install `spaCy`. (The latest version is 2.3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: spacy in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (2.3.2)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy) (1.19.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy) (49.3.2.post20200812)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy) (4.48.2)\n",
      "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: thinc==7.4.1 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy) (7.4.1)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spacy` processes text with the aid of **models**, which are files containing numerical weights derived from neural networks. These networks are trained on linguistic and textual features like parts-of-speech and named entities (which we'll discuss below). \n",
    "\n",
    "A number of models are available for languages other than English. See the [spaCy documentation](https://spacy.io/usage/models) for more information. You can also train your own models, if you are working with special kinds of texts. \n",
    "\n",
    "But in this workshop, we'll use one of the pre-trained models for parsing English. Because the models are rather larger, they require downloading separately. \n",
    "\n",
    "We'll use the \"medium\" model for English, because the \"small\" model lacks some features it will be useful to explore. For your own projects, you might want to try the \"large\" model, which is the most fully featured and most accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_md==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.1/en_core_web_md-2.3.1.tar.gz#egg=en_core_web_md==2.3.1 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from en_core_web_md==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (4.48.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (49.3.2.post20200812)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: thinc==7.4.1 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.19.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/py38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2020.6.20)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import `spacy` and load our model. This may take a few moments.\n",
    "\n",
    "If you get an error on loading the module, try this instead:\n",
    "\n",
    "```\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also import some other Python libraries that will be helpful for our analysis. \n",
    "\n",
    "`pandas`, `numpy`, `matplotlib`, and `requests` may already be installed, if you're using a Colab notebook or an Anaconda distribution of Python. Otherwise, you can install them first as follows:\n",
    "\n",
    "```\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install requests\n",
    "```\n",
    "\n",
    "The `collections` library should be part of the standard Python installation (no need to install separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data\n",
    "\n",
    "Now we need some textual data to work with. NLP generally works best on clean texts without typos, special characters, etc. But those often aren't the kinds of text we want to process. To simulate something more interesting, we're going to use a Twitter dataset in this workshop.\n",
    "\n",
    "The tweets in this dataset are from the official accounts of members of the United States Senate, collected between January 1, 2020 and May 7, 2020 by GW LAI's Social Feed Manager project and available on [TweetSets](https://tweetsets.library.gwu.edu/). \n",
    "\n",
    "I have downsampled this dataset to a size more manageable for live coding. The initial dataset contained 40,000 tweets. I have also removed most of the metadata from the original tweets, keeping just a few fields (in addition to the full text). \n",
    "\n",
    "If you wish to replicate my extraction and sampling process, see [this notebook](). **ADD LINK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `requests` library to fetch the tweet data as a JSON file and convert it to Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('https://raw.githubusercontent.com/gwu-libraries/gwlibraries-workshops/master/text-analysis-python/senate-tweetset-sample-2020.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data is a list of dictionaries, and that the text of each tweet is accessible via the `full_text` key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the text\n",
    "\n",
    "Processing a text with Spacy will an object called a `Doc`. This process is computationally expensive, though spaCy is highly optimized, and it includes the special `.pipe` method for handling multiple texts in parallel.\n",
    "\n",
    "**Note** When I say \"text,\" I mean a Python **string**. spaCy cannot process other Python data types. \n",
    "\n",
    "We can pass a list of texts to `.pipe` instead of iterating over them with a `for` loop, and we will get back a collection of `Doc` objects.\n",
    "\n",
    "Here I'm using a list comprehension to create a list of just the strings in the `full_text` field of each tweet. Then I pass that list to `nlp.pipe`. I wrap the latter in `list` because technically, `.pipe` returns an iterable, which we can loop over, but converting it to a list will allow us to get specific elements by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [tweet['full_text'] for tweet in tweets]\n",
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should have taken more than a few seconds, unless you have a very fast machine. But now our texts are processed, and we can start to explore their structure.\n",
    "\n",
    "**Important** I did _not_ overwrite my `tweets` variable with the processed versions of the tweets, because the original dataset contains important metadata (like each tweet's author, number of times it was retweeted, etc.). When processing texts, you'll generally need to keep the metadata separate. So now we have two lists, `tweets` and `docs`. As long as we keep the two lists in tact and in the same order, we can easily reference back from the second to the first (e.g., to associate a given tweet's contents with its creator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP fundamentals\n",
    "\n",
    "There are a few key components of a `Doc` that we will review. \n",
    "\n",
    "#### Tokens\n",
    "\n",
    "A token is a unique set of characters obtained from a larger string by discarding some elements of the string. That definition is vague on purpose, because **tokenization** can be achieved in different ways. Let's see if we can determine how spaCy tokenizes. \n",
    "\n",
    "Note that if I just inspect a spaCy `doc`, it doesn't look much different from a string. But to inspect the tokens in it, I can simply wrap it in Python's `list` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kudos to @Toyota as its workforce prepares to return to work at TMMTX (San Antonio) on Monday, May 4.  Their \"Safe at Work Playbook\" is based on guidelines from the CDC, WHO, and OSHA, best practices developed by Toyota Working Groups, and local orders and other authorities."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Kudos,\n",
       " to,\n",
       " @Toyota,\n",
       " as,\n",
       " its,\n",
       " workforce,\n",
       " prepares,\n",
       " to,\n",
       " return,\n",
       " to,\n",
       " work,\n",
       " at,\n",
       " TMMTX,\n",
       " (,\n",
       " San,\n",
       " Antonio,\n",
       " ),\n",
       " on,\n",
       " Monday,\n",
       " ,,\n",
       " May,\n",
       " 4,\n",
       " .,\n",
       "  ,\n",
       " Their,\n",
       " \",\n",
       " Safe,\n",
       " at,\n",
       " Work,\n",
       " Playbook,\n",
       " \",\n",
       " is,\n",
       " based,\n",
       " on,\n",
       " guidelines,\n",
       " from,\n",
       " the,\n",
       " CDC,\n",
       " ,,\n",
       " WHO,\n",
       " ,,\n",
       " and,\n",
       " OSHA,\n",
       " ,,\n",
       " best,\n",
       " practices,\n",
       " developed,\n",
       " by,\n",
       " Toyota,\n",
       " Working,\n",
       " Groups,\n",
       " ,,\n",
       " and,\n",
       " local,\n",
       " orders,\n",
       " and,\n",
       " other,\n",
       " authorities,\n",
       " .]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare that with what we get by simply splitting the **string** version of this text on whitespace with Python's builtin `.split` method. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kudos',\n",
       " 'to',\n",
       " '@Toyota',\n",
       " 'as',\n",
       " 'its',\n",
       " 'workforce',\n",
       " 'prepares',\n",
       " 'to',\n",
       " 'return',\n",
       " 'to',\n",
       " 'work',\n",
       " 'at',\n",
       " 'TMMTX',\n",
       " '(San',\n",
       " 'Antonio)',\n",
       " 'on',\n",
       " 'Monday,',\n",
       " 'May',\n",
       " '4.',\n",
       " 'Their',\n",
       " '\"Safe',\n",
       " 'at',\n",
       " 'Work',\n",
       " 'Playbook\"',\n",
       " 'is',\n",
       " 'based',\n",
       " 'on',\n",
       " 'guidelines',\n",
       " 'from',\n",
       " 'the',\n",
       " 'CDC,',\n",
       " 'WHO,',\n",
       " 'and',\n",
       " 'OSHA,',\n",
       " 'best',\n",
       " 'practices',\n",
       " 'developed',\n",
       " 'by',\n",
       " 'Toyota',\n",
       " 'Working',\n",
       " 'Groups,',\n",
       " 'and',\n",
       " 'local',\n",
       " 'orders',\n",
       " 'and',\n",
       " 'other',\n",
       " 'authorities.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]['full_text'].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's built-in `split` command separates a string based on a single character or character-combination at a time. By default, it splits on whitespace. For English text, this leaves punctuation marks attached to the words they preceded or follow. \n",
    "\n",
    "Using regular expressions, it's possible to create more complex string separations with the `re` library in Python. But that's still not an easy task, given the occurence of tokens that actually contain punctuation: for example, _U.S._ or _U.K._ spaCy includes recipes to account for such tokens in its tokenizing routine.\n",
    "\n",
    "**Note** spaCy's tokenization performs quite well as a general rule, at least on what we might call standard English text. But if you're working with text that is non-standard or just messy, it may not be as accurate. As with most of its functionality, it's possible to customize spaCy's tokenizer to handle special cases, but that's beyond the scope of this workshop. See the [dcumentation](https://spacy.io/usage/linguistic-features#tokenization) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords, punctuation, and URL's\n",
    "\n",
    "If you compare the two lists above, you'll see that in the list derived from our spaCy `doc`, the elements are not surrounded by single quotes. This is a signal that a spaCy `Token` is not a Python `str`. But like Python strings, which have special methods like `split` built-in, tokens have their own methods and properties. \n",
    "\n",
    "For example, spaCy attempts to mark URL's and email addresses as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Many @SocialSecurity beneficiaries were surprised by a recent @IRSnews rule that required a tax return to receive direct #COVID19 checks. My colleagues and I urged @USTreasury to waive this burdensome requirement.\n",
       " \n",
       "Tonight, this rule was reversed. https://t.co/Zx4jujdris"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The last token in the second doc is a URL.\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1][-1].like_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use some of the token's properties to identify what we might call \"content\" words in our text, preparatory to performing some form of semantic analysis. We'll most likely want to filter out punctuation. We'll also probably want to filter out **stopwords**, which in English include articles (_a_, _an_, and _the_), conjunctions (_and_, _or_), prepositions (_of_, _in_), and the like. \n",
    "\n",
    "Let's write a function that accepts a spaCy `Document` object as its argument and returns only those tokens that are neither punctuation, whitespace, stopwords, nor URL's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(doc):\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # We include is_space because even though the default tokenization ignores the space between words, extra spaces\n",
    "        # like line breaks can register as distinct tokens\n",
    "        if not token.is_stop and not token.is_space and not token.is_punct and not token.like_url:\n",
    "            tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Kudos,\n",
       " @Toyota,\n",
       " workforce,\n",
       " prepares,\n",
       " return,\n",
       " work,\n",
       " TMMTX,\n",
       " San,\n",
       " Antonio,\n",
       " Monday,\n",
       " 4,\n",
       " Safe,\n",
       " Work,\n",
       " Playbook,\n",
       " based,\n",
       " guidelines,\n",
       " CDC,\n",
       " OSHA,\n",
       " best,\n",
       " practices,\n",
       " developed,\n",
       " Toyota,\n",
       " Working,\n",
       " Groups,\n",
       " local,\n",
       " orders,\n",
       " authorities]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stops(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's not perfect. In the original text, \"Monday, May 4\" is given as a date. Our function kept the \"4\" but discarded \"May\": it doesn't have a way to distinguish _May_ the month from _may_ the auxillary verb, which is a stopword. You can augment or even replace spaCy's built-in list of stopwords with your own. For instance, if your text has a lot of dates in it, you may want to remove _may_ from the list.\n",
    "\n",
    "We could also weed out tokens like \"4\" by checking the `Token.is_digit` and/or `Token.like_num` flags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmas and parts-of-speech\n",
    "\n",
    "We'll use our `remove_stops` function a bit later. Now let's look at some properties of tokens that we might want to analyze across our collection of documents.\n",
    "\n",
    "spaCy is more than just a tokenizer. When we pass a string to the `nlp` function, it analyzes the text using a series of models. One of these models tags every token with its grammatical part of speech (POS). The models are probabilistic, so depending on the nature of the text, the results may be more or less accurate.\n",
    "\n",
    "We can view the POS tags using the `.pos_` attribute of any given token. (Note the underscore at the end of the attribute!) Definitions of the tags are available on the website of the [Universal Dependencies project](https://universaldependencies.org/docs/u/pos/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Kudos': 'NOUN',\n",
       " 'to': 'ADP',\n",
       " '@Toyota': 'PUNCT',\n",
       " 'as': 'SCONJ',\n",
       " 'its': 'DET',\n",
       " 'workforce': 'NOUN',\n",
       " 'prepares': 'VERB',\n",
       " 'return': 'VERB',\n",
       " 'work': 'NOUN',\n",
       " 'at': 'ADP',\n",
       " 'TMMTX': 'PROPN',\n",
       " '(': 'PUNCT',\n",
       " 'San': 'PROPN',\n",
       " 'Antonio': 'PROPN',\n",
       " ')': 'PUNCT',\n",
       " 'on': 'ADP',\n",
       " 'Monday': 'PROPN',\n",
       " ',': 'PUNCT',\n",
       " 'May': 'PROPN',\n",
       " '4': 'NUM',\n",
       " '.': 'PUNCT',\n",
       " ' ': 'SPACE',\n",
       " 'Their': 'DET',\n",
       " '\"': 'PUNCT',\n",
       " 'Safe': 'ADJ',\n",
       " 'Work': 'PROPN',\n",
       " 'Playbook': 'PROPN',\n",
       " 'is': 'AUX',\n",
       " 'based': 'VERB',\n",
       " 'guidelines': 'NOUN',\n",
       " 'from': 'ADP',\n",
       " 'the': 'DET',\n",
       " 'CDC': 'PROPN',\n",
       " 'WHO': 'PROPN',\n",
       " 'and': 'CCONJ',\n",
       " 'OSHA': 'PROPN',\n",
       " 'best': 'ADJ',\n",
       " 'practices': 'NOUN',\n",
       " 'developed': 'VERB',\n",
       " 'by': 'ADP',\n",
       " 'Toyota': 'PROPN',\n",
       " 'Working': 'PROPN',\n",
       " 'Groups': 'PROPN',\n",
       " 'local': 'ADJ',\n",
       " 'orders': 'NOUN',\n",
       " 'other': 'ADJ',\n",
       " 'authorities': 'NOUN'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a dictionary comprehension to view the .pos_ attribute of the tokens in a spaCy doc, along with the token's string representation\n",
    "{token.text: token.pos_ for token in docs[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the part of speech, another useful piece of token metadata is the _lemma_ of each word, which is a sort of normalized form of it intended to make comparison between different grammatical inflections (plurals, verb tense, etc.) easy to compare. We can access it via the `Token.lemma_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Kudos': 'kudo',\n",
       " 'to': 'to',\n",
       " '@Toyota': '@Toyota',\n",
       " 'as': 'as',\n",
       " 'its': '-PRON-',\n",
       " 'workforce': 'workforce',\n",
       " 'prepares': 'prepare',\n",
       " 'return': 'return',\n",
       " 'work': 'work',\n",
       " 'at': 'at',\n",
       " 'TMMTX': 'TMMTX',\n",
       " '(': '(',\n",
       " 'San': 'San',\n",
       " 'Antonio': 'Antonio',\n",
       " ')': ')',\n",
       " 'on': 'on',\n",
       " 'Monday': 'Monday',\n",
       " ',': ',',\n",
       " 'May': 'May',\n",
       " '4': '4',\n",
       " '.': '.',\n",
       " ' ': ' ',\n",
       " 'Their': '-PRON-',\n",
       " '\"': '\"',\n",
       " 'Safe': 'safe',\n",
       " 'Work': 'Work',\n",
       " 'Playbook': 'Playbook',\n",
       " 'is': 'be',\n",
       " 'based': 'base',\n",
       " 'guidelines': 'guideline',\n",
       " 'from': 'from',\n",
       " 'the': 'the',\n",
       " 'CDC': 'CDC',\n",
       " 'WHO': 'WHO',\n",
       " 'and': 'and',\n",
       " 'OSHA': 'OSHA',\n",
       " 'best': 'good',\n",
       " 'practices': 'practice',\n",
       " 'developed': 'develop',\n",
       " 'by': 'by',\n",
       " 'Toyota': 'Toyota',\n",
       " 'Working': 'Working',\n",
       " 'Groups': 'Groups',\n",
       " 'local': 'local',\n",
       " 'orders': 'order',\n",
       " 'other': 'other',\n",
       " 'authorities': 'authority'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{token.text: token.lemma_ for token in docs[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that lemmatization in spaCy leaves proper nouns (like the _Groups_ in _Toyota Working Groups_) alone. But _practiced_ becomes _practice_, _authorities_ becomes _authority_, and _best_ becomes _good_. \n",
    "\n",
    "Using lemmas and POS, we can begin to look at our dataset as **a collection** of texts, rather than text by text. \n",
    "\n",
    "Let's write a function to find the most frequently used words by Senators in their tweets, organized by part of speech. Our function should accept a list of spaCy `Document`s and return a dictionary of POS tags, where each POS tag maps to a collection of associated lemmas and their frequencies. For instance, we should be able to write `pos_dict['ADJ']` and see all the unique adjectives in the collection and how frequently they occur.\n",
    "\n",
    "To do this, we'll use a couple of special Python types from the `collections` module. (We imported them earlier.)\n",
    "- `defaultdict` creates a Python dictionary whose **values** are initialized as another Python collection type, such as a list or a dictionary. `defaultdict` provides a convenient way to make a nested data structure.\n",
    "- `Counter` is a Python dictionary that initializes every value to 0. It's useful for counting a collection of objects (in this case, our lemmas.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos(docs):\n",
    "    # Initialize the data structure, which will be a dictionary of Counter-type dictionaries\n",
    "    pos_dict = defaultdict(Counter)\n",
    "    # Loop over the documents\n",
    "    for doc in docs:\n",
    "        # In each doc, loop over the tokens\n",
    "        for token in doc:\n",
    "            # Get the POS label\n",
    "            pos = token.pos_\n",
    "            # Get the lemman\n",
    "            lemma = token.lemma_\n",
    "            # Update the nested dictionary, incrementing the inner dictionary by one for this lemma\n",
    "            pos_dict[pos][lemma] += 1\n",
    "    # Don't forget to return something!\n",
    "    return pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = count_pos(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at `pos_dict['ADJ']`, we'll see all the adjectival lemmas in our dataset, which is a lot. But because the inner dictionary is a Python `Counter`, we can use its `most_common` method to find the top N lemmas under a given part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('more', 715),\n",
       " ('small', 456),\n",
       " ('great', 356),\n",
       " ('good', 346),\n",
       " ('new', 345),\n",
       " ('federal', 336),\n",
       " ('american', 326),\n",
       " ('public', 312),\n",
       " ('many', 304),\n",
       " ('important', 279),\n",
       " ('safe', 262),\n",
       " ('first', 241),\n",
       " ('critical', 226),\n",
       " ('proud', 223),\n",
       " ('economic', 223),\n",
       " ('last', 222),\n",
       " ('bipartisan', 221),\n",
       " ('local', 206),\n",
       " ('sure', 199),\n",
       " ('other', 198)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_dict['ADJ'].most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's perhaps not surprising that a number of these are governance-related words, like _federal_ and _economic_ and _bipartisan_. There are also a fair number of **positive** adjectives: _good_, _great_, _proud_, _important_. I'm actually a bit surprised not to see more **negative** adjectives in this list, given the fractious nature of Congressional politics at the moment. \n",
    "\n",
    "At any rate, it's a tiny example of the kind of quantitative exploration you can do with NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entities\n",
    "\n",
    "So far we've looked at **syntactic** features of our dataset. spaCy also includes some tools that allow us to look at the **semantic** information in an quantitative way. It's worth pointing out (again) that semantic computational analysis is quite challenging and remains a very active area of research. spaCy isn't necessarily intended to be used on its own for this work, but rather as a pre-processing tool to feed into other kinds of tools and models capable of more sophisticated analysis.\n",
    "\n",
    "One kind of semantic analysis identifies the named entities in a collection of documents. These are words or phrases that refer to people, places, organizations, etc. Because such names can be either single words or phrases, it's not sufficient to identify the proper nouns in a text. spaCy uses a special probabilistic model to extract and classify named entities. Let's see how accurate it is.\n",
    "\n",
    "A`Document` has a property call `ents` that returns only the named entities recognized for that document. Each entity has a `label_` attribute that identifies [the classification](https://spacy.io/api/annotation#named-entities) assigned to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use a dictionary comprehension to create a dictionary of all the entities in the documents in our dataset\n",
    "entities = {ent.text: ent.label_ for doc in docs for ent in doc.ents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TMMTX': 'ORG',\n",
       " 'San Antonio': 'GPE',\n",
       " 'Monday': 'DATE',\n",
       " 'May 4': 'DATE',\n",
       " 'CDC': 'ORG',\n",
       " 'OSHA': 'ORG',\n",
       " 'Toyota Working Groups': 'ORG',\n",
       " 'Tonight': 'TIME',\n",
       " 'a year': 'DATE',\n",
       " 'House': 'ORG',\n",
       " 'Mitch McConnell': 'PERSON',\n",
       " 'GOP': 'ORG',\n",
       " 'Senate': 'ORG',\n",
       " 'Kansas': 'GPE',\n",
       " '@kscosmosphere': 'ORG',\n",
       " 'PPP': 'ORG',\n",
       " '&amp': 'ORG',\n",
       " 'https://t.co/0tPRHeIXVI': 'ORG',\n",
       " 'Trump Administration': 'ORG',\n",
       " 'EPA': 'ORG',\n",
       " 'AZ': 'LOC',\n",
       " 'Jack': 'PERSON',\n",
       " 'Scottsdale': 'GPE',\n",
       " 'Rudy &amp': 'ORG',\n",
       " 'First': 'ORDINAL',\n",
       " 'the United States': 'GPE',\n",
       " 'More than $67 billion': 'MONEY',\n",
       " 'OpportunityZones': 'MONEY',\n",
       " 'Bernie Sanders': 'PERSON',\n",
       " 'Donald Trump': 'PERSON',\n",
       " 'Texas': 'GPE',\n",
       " 'last night': 'TIME',\n",
       " 'Russia': 'GPE',\n",
       " '2,600 mile': 'QUANTITY',\n",
       " 'China': 'GPE',\n",
       " 'El Al': 'LOC',\n",
       " 'Israel &amp': 'ORG',\n",
       " 'Air France': 'ORG',\n",
       " 'China &amp': 'ORG',\n",
       " 'France': 'GPE',\n",
       " 'The United States': 'GPE',\n",
       " 'U.S.': 'GPE',\n",
       " 'the Senate HELP Committee': 'ORG',\n",
       " 'the Coronavirus Aid': 'ORG',\n",
       " 'Economic Security': 'ORG',\n",
       " 'the U.S. Senate': 'ORG',\n",
       " '@USDOT': 'ORG',\n",
       " 'Ted Cruz': 'PERSON',\n",
       " 'Democrats': 'NORP',\n",
       " 'Hunter Biden Testimony Now': 'PERSON',\n",
       " 'Washington': 'GPE',\n",
       " 'February': 'DATE',\n",
       " 'Nevada Woman': 'ORG',\n",
       " 'STEM': 'ORG',\n",
       " 'Isabelle West': 'LOC',\n",
       " 'Isabelle': 'PERSON',\n",
       " 'Truckee Meadows Community College &amp': 'ORG',\n",
       " 'Tesla': 'ORG',\n",
       " 'Gigafactory': 'PERSON',\n",
       " 'Reno': 'GPE',\n",
       " 'Hawley': 'PERSON',\n",
       " '4': 'CARDINAL',\n",
       " 'today': 'DATE',\n",
       " 'New Hampshire': 'GPE',\n",
       " '@HHSgov': 'ORG',\n",
       " '3': 'CARDINAL',\n",
       " 'Louisiana': 'GPE',\n",
       " 'the end of April': 'DATE',\n",
       " '@WSJ': 'ORG',\n",
       " '@fredpersinger2': 'ORG',\n",
       " 'about #': 'CARDINAL',\n",
       " 'West Virginians': 'NORP',\n",
       " 'around 9:35 a.m.': 'TIME',\n",
       " 'Georgia': 'GPE',\n",
       " 'Arkansas &amp': 'ORG',\n",
       " 'America': 'GPE',\n",
       " '@cabotsd': 'GPE',\n",
       " 'SendTeachersLove': 'MONEY',\n",
       " 'The American Legion Department of Nevada': 'ORG',\n",
       " 'the Silver State': 'GPE',\n",
       " 'Alexa Negrón Luciano': 'PERSON',\n",
       " 'Valentine': 'PERSON',\n",
       " 'Archie': 'PERSON',\n",
       " 'NBA': 'ORG',\n",
       " 'MLS': 'GPE',\n",
       " 'Houston Rodeo': 'GPE',\n",
       " 'SXSW': 'GPE',\n",
       " 'Graham': 'PERSON',\n",
       " 'DOJ': 'ORG',\n",
       " 'Roger Stone’s': 'PERSON',\n",
       " 'American': 'NORP',\n",
       " 'Trump': 'ORG',\n",
       " 'our Justice Department': 'ORG',\n",
       " 'RIP Tom Coburn': 'PERSON',\n",
       " 'Tom': 'PERSON',\n",
       " 'Doc Coburn’s': 'PERSON',\n",
       " 'POTUS': 'ORG',\n",
       " '@CDCgov': 'ORG',\n",
       " 'the Paycheck Protection Program': 'ORG',\n",
       " 'RNC': 'ORG',\n",
       " 'one': 'CARDINAL',\n",
       " '2020Census': 'MONEY',\n",
       " 'Chag Sameach': 'PERSON',\n",
       " 'Passover': 'EVENT',\n",
       " 'tonight': 'TIME',\n",
       " 'Team Warner': 'ORG',\n",
       " 'Delaware Collrge of Art and Design': 'PERSON',\n",
       " 'Jean Dalhgren': 'PERSON',\n",
       " 'Delaware': 'GPE',\n",
       " 'tomorrow': 'DATE',\n",
       " 'St. Paul': 'PERSON',\n",
       " '@NRateliff &amp': 'ORG',\n",
       " 'the Night Sweats': 'WORK_OF_ART',\n",
       " '23': 'CARDINAL',\n",
       " 'David': 'PERSON',\n",
       " '@OTR_1974 &amp': 'ORG',\n",
       " 'Midtown Crossing Apartments': 'FAC',\n",
       " '🚨': 'NORP',\n",
       " 'Idaho': 'GPE',\n",
       " '8,846': 'CARDINAL',\n",
       " '1,399,191,164': 'MONEY',\n",
       " 'Paycheck Protection Program': 'ORG',\n",
       " 'Native': 'NORP',\n",
       " 'Indian Country': 'GPE',\n",
       " 'Congress': 'ORG',\n",
       " 'Families &amp': 'ORG',\n",
       " '$569M': 'MONEY',\n",
       " '@BennieGThompson &amp': 'ORG',\n",
       " '@WhiteHouse': 'ORG',\n",
       " 'Cascade County': 'GPE',\n",
       " 'Joe Briggs': 'PERSON',\n",
       " 'MT ag': 'ORG',\n",
       " '🇸': 'ORG',\n",
       " 'Today': 'DATE',\n",
       " '@michaeljknowles': 'ORG',\n",
       " '42': 'CARDINAL',\n",
       " 'only days': 'DATE',\n",
       " '@SenJackyRosen &amp': 'ORG',\n",
       " '@SenJohnBarrasso': 'ORG',\n",
       " 'rural hospitals &amp': 'ORG',\n",
       " '➡': 'PRODUCT',\n",
       " '32': 'CARDINAL',\n",
       " '39': 'CARDINAL',\n",
       " 'Small Business Administration': 'ORG',\n",
       " 'learning &amp': 'ORG',\n",
       " '@USDA': 'ORG',\n",
       " '.@SenateDems &amp': 'ORG',\n",
       " 'WA &amp': 'ORG',\n",
       " '$10 billion': 'MONEY',\n",
       " 'half': 'CARDINAL',\n",
       " 'every week': 'DATE',\n",
       " '@BauerHockey': 'NORP',\n",
       " 'Massachusetts': 'GPE',\n",
       " '@USTreasury': 'ORG',\n",
       " 'Oregon': 'GPE',\n",
       " 'Chinese': 'NORP',\n",
       " 'Coronavirus': 'ORG',\n",
       " 'second': 'ORDINAL',\n",
       " '25,000': 'MONEY',\n",
       " 'Chris Koos': 'PERSON',\n",
       " '@BNEDC': 'ORG',\n",
       " 'Bloomington': 'GPE',\n",
       " 'the days': 'DATE',\n",
       " 'Springfield': 'GPE',\n",
       " 'Frank': 'PERSON',\n",
       " 'Cinda Edwards': 'PERSON',\n",
       " 'El pres': 'GPE',\n",
       " 'del comité Rubio lanzó un documento de preguntas': 'PERSON',\n",
       " 'sobre el #': 'PERSON',\n",
       " 'Los préstamos PPP': 'GPE',\n",
       " 'Lea más aquí ⬇': 'PERSON',\n",
       " '2018': 'DATE',\n",
       " 'Philadelphia': 'GPE',\n",
       " 'the Catholic Social Services': 'ORG',\n",
       " 'Fulton': 'ORG',\n",
       " 'Mientras las escuelas de Arizona': 'PERSON',\n",
       " 'de marzo': 'GPE',\n",
       " 'los distritos': 'GPE',\n",
       " 'están trabajando para alimentar': 'ORG',\n",
       " 'los niños': 'GPE',\n",
       " 'Aquí es como': 'PERSON',\n",
       " 'Jay Sekulow': 'PERSON',\n",
       " '@realDonaldTrump': 'ORG',\n",
       " 'COVID-19 &amp': 'ORG',\n",
       " 'weeks': 'DATE',\n",
       " 'Admin': 'PERSON',\n",
       " 'Jewish': 'NORP',\n",
       " 'first': 'ORDINAL',\n",
       " 'Samuel': 'PERSON',\n",
       " 'Clinton': 'PERSON',\n",
       " '@SBAgov': 'ORG',\n",
       " 'MT': 'ORG',\n",
       " 'season': 'DATE',\n",
       " 'Jeff Bezos': 'PERSON',\n",
       " '$34 billion': 'MONEY',\n",
       " 'March 12th': 'DATE',\n",
       " 'two weeks': 'DATE',\n",
       " 'Amazon Workers International': 'ORG',\n",
       " 'https://t.co/ce1HZrneuY': 'ORG',\n",
       " 'Tuesday': 'DATE',\n",
       " 'Hoosier': 'GPE',\n",
       " 'SBA': 'ORG',\n",
       " '#PaycheckProtectionProgram and': 'ORG',\n",
       " 'Indiana': 'GPE',\n",
       " '#ICYMI': 'PERSON',\n",
       " 'the China Virus': 'ORG',\n",
       " 'a wonderful day': 'DATE',\n",
       " 'Jeff': 'PERSON',\n",
       " '55 years': 'DATE',\n",
       " 'Selma': 'GPE',\n",
       " 'Montgomery': 'PERSON',\n",
       " 'NIMBUS Lab': 'ORG',\n",
       " 'ag': 'ORG',\n",
       " 'NEXTT': 'ORG',\n",
       " '#LoveNE': 'PERSON',\n",
       " 'over $230 million': 'MONEY',\n",
       " 'CARES Act': 'LAW',\n",
       " '28': 'PRODUCT',\n",
       " 'Nevada': 'GPE',\n",
       " 'Treasury': 'ORG',\n",
       " 'Missouri': 'GPE',\n",
       " '32%': 'PERCENT',\n",
       " 'the Defense Production Act': 'LAW',\n",
       " '100': 'CARDINAL',\n",
       " 'Constitution &amp': 'ORG',\n",
       " 'the Born-Alive Abortion Survivors Protection Act': 'ORG',\n",
       " '2': 'CARDINAL',\n",
       " 'Martin Luther King Jr. Scholarship Breakfast': 'PERSON',\n",
       " 'Cleveland': 'GPE',\n",
       " 'https://t.co/jZZZxawDZS': 'CARDINAL',\n",
       " 'the 47th anniversary': 'DATE',\n",
       " 'RoevWade': 'MONEY',\n",
       " '#RoeIRL https://t.co/LwNP2fm7oA': 'PERSON',\n",
       " '@SenatorCollins': 'ORG',\n",
       " 'Collins': 'PERSON',\n",
       " 'Mick Mulvaney': 'PERSON',\n",
       " 'John Bolton': 'PERSON',\n",
       " 'Ukraine': 'GPE',\n",
       " 'MSNBC': 'ORG',\n",
       " 'yesterday': 'DATE',\n",
       " 'Missoula': 'GPE',\n",
       " 'Montanans': 'NORP',\n",
       " 'the 21st century': 'DATE',\n",
       " 'CoronaVirus': 'ORG',\n",
       " 'the end of the week': 'DATE',\n",
       " 'millions': 'CARDINAL',\n",
       " '#ProtectOurCare': 'MONEY',\n",
       " 'the CARES Act': 'LAW',\n",
       " 'two weeks later': 'DATE',\n",
       " '@RonWyden': 'ORG',\n",
       " 'ASAP': 'GPE',\n",
       " \"New Mexico's\": 'GPE',\n",
       " 'Vought': 'PERSON',\n",
       " 'Tribally': 'GPE',\n",
       " 'Deadwood': 'GPE',\n",
       " 'South Dakota': 'GPE',\n",
       " 'summer': 'DATE',\n",
       " 'D.C.': 'GPE',\n",
       " 'Pennsylvania': 'GPE',\n",
       " 'March 1': 'DATE',\n",
       " 'state &amp': 'ORG',\n",
       " 'communities safe': 'ORG',\n",
       " 'SmallBiz': 'PRODUCT',\n",
       " '11 a.m. to 1 p.m.': 'TIME',\n",
       " '1/2': 'CARDINAL',\n",
       " '@IvankaTrump': 'ORG',\n",
       " 'the Vietnam War': 'EVENT',\n",
       " 'five': 'CARDINAL',\n",
       " 'Pelosi': 'PERSON',\n",
       " 'New Year': 'EVENT',\n",
       " 'this year': 'DATE',\n",
       " 'White House': 'ORG',\n",
       " 'West Virginia': 'GPE',\n",
       " 'Carranza': 'PERSON',\n",
       " 'Chambers of Commerce &amp': 'ORG',\n",
       " '501(c)(6': 'CARDINAL',\n",
       " '&gt': 'FAC',\n",
       " 'Cocina Madrigal': 'PERSON',\n",
       " 'Indian Gardens Cafe &amp': 'ORG',\n",
       " '@GovKemp': 'ORG',\n",
       " 'PPE': 'ORG',\n",
       " 'Mike': 'PERSON',\n",
       " \"Donald Trump's\": 'PERSON',\n",
       " 'this Monday': 'DATE',\n",
       " 'TeamWarren': 'PERSON',\n",
       " 'Better Angels': 'ORG',\n",
       " 'Sam Neff &amp': 'PERSON',\n",
       " 'Jacey Roblee': 'PERSON',\n",
       " 'Jacey': 'PERSON',\n",
       " 'Alabama': 'GPE',\n",
       " 'over $50M': 'MONEY',\n",
       " 'FAFSA': 'ORG',\n",
       " '3 years': 'DATE',\n",
       " '#MuslimBan': 'PERSON',\n",
       " 'NoBanAct': 'MONEY',\n",
       " '@SEIUhciimk': 'ORG',\n",
       " 'Greg Kelley': 'PERSON',\n",
       " 'Hill': 'ORG',\n",
       " 'Cameroon': 'GPE',\n",
       " 'more than 3,000': 'CARDINAL',\n",
       " '2017': 'DATE',\n",
       " 'Pres': 'PERSON',\n",
       " 'Biya': 'PERSON',\n",
       " 'last month’s': 'DATE',\n",
       " 'Ngarbu': 'GPE',\n",
       " 'Republican': 'NORP',\n",
       " 'Chuck Schumer': 'PERSON',\n",
       " 'The Idaho Congressional Delegation': 'ORG',\n",
       " 'Military Service Academy Days': 'ORG',\n",
       " 'TODAY': 'DATE',\n",
       " 'Lewiston': 'GPE',\n",
       " 'Elaine McCusker': 'PERSON',\n",
       " 'Two': 'CARDINAL',\n",
       " 'Rhode Island': 'GPE',\n",
       " '@ChuckGrassley &amp': 'ORG',\n",
       " 'Last year': 'DATE',\n",
       " 'Murray': 'PERSON',\n",
       " 'Paycheck Fairness Act': 'LAW',\n",
       " 'Kem Krest': 'PERSON',\n",
       " 'Elkhart': 'GPE',\n",
       " 'Hoosiers': 'ORG',\n",
       " 'the school year': 'DATE',\n",
       " 'the final days': 'DATE',\n",
       " '@SaintAnselm': 'FAC',\n",
       " '@WMUR9': 'ORG',\n",
       " '@nhdems McIntyre-Shaheen': 'ORG',\n",
       " '#nhpolitics': 'PERSON',\n",
       " 'The Maryland Delegation': 'ORG',\n",
       " '@FEMA': 'ORG',\n",
       " 'State': 'ORG',\n",
       " 'Baltimore': 'GPE',\n",
       " 'Maryland': 'GPE',\n",
       " '300': 'CARDINAL',\n",
       " '1,000': 'CARDINAL',\n",
       " 'Putin': 'PERSON',\n",
       " 'Alaska': 'GPE',\n",
       " 'Crimea': 'LOC',\n",
       " 'Constitution': 'LAW',\n",
       " 'Medicare': 'ORG',\n",
       " 'the South Bend - Elkhart Regional Partnership': 'ORG',\n",
       " 'SOTU': 'PERSON',\n",
       " '#COVID19': 'PERSON',\n",
       " 'PaycheckProtectionProgram Congress': 'ORG',\n",
       " 'https://t.co/zzsbwlZHPa': 'CARDINAL',\n",
       " 'SCOTUS': 'ORG',\n",
       " 'Gorsuch': 'PERSON',\n",
       " 'Kavanaugh': 'PERSON',\n",
       " '@fema': 'ORG',\n",
       " 'Next year': 'DATE',\n",
       " 'Americans': 'NORP',\n",
       " 'this morning': 'TIME',\n",
       " 'around 8:40 a.m.': 'TIME',\n",
       " '@GovernorKayIvey': 'GPE',\n",
       " 'Beijing': 'GPE',\n",
       " 'EarthDay': 'MONEY',\n",
       " 'Mary Jane Fate': 'PERSON',\n",
       " 'Alaskan': 'NORP',\n",
       " 'Athabaskan': 'NORP',\n",
       " 'Alaska Native': 'ORG',\n",
       " 'Census': 'EVENT',\n",
       " 'WV': 'GPE',\n",
       " '#BeCounted': 'PERSON',\n",
       " 'Walter Scott': 'PERSON',\n",
       " '@SenTomCotton &amp': 'ORG',\n",
       " '@ahahospitals': 'ORG',\n",
       " 'Richard Pollack': 'PERSON',\n",
       " 'US': 'GPE',\n",
       " 'Communist': 'NORP',\n",
       " 'tens of millions': 'CARDINAL',\n",
       " 'Whitmer': 'PERSON',\n",
       " 'Michigan': 'GPE',\n",
       " 'Evers': 'PERSON',\n",
       " 'Wisconsin': 'GPE',\n",
       " 'Wolf': 'PERSON',\n",
       " '9th': 'ORDINAL',\n",
       " 'Standing on the Shoulders of Giants': 'WORK_OF_ART',\n",
       " '#BlackHistoryMonth Celebration': 'PRODUCT',\n",
       " '📅 February 9, 2020\\n': 'DATE',\n",
       " 'Lawnside': 'PRODUCT',\n",
       " 'NJ': 'GPE',\n",
       " '🎤 Keynote': 'ORG',\n",
       " '@GovJanetMills': 'ORG',\n",
       " 'Maine': 'GPE',\n",
       " 'workers &amp': 'ORG',\n",
       " 'three': 'CARDINAL',\n",
       " 'Ethiopia': 'GPE',\n",
       " '@WHO': 'ORG',\n",
       " 'Cruz': 'PERSON',\n",
       " 'Obama': 'ORG',\n",
       " 'Republicans': 'NORP',\n",
       " 'Bolton': 'PERSON',\n",
       " 'two': 'CARDINAL',\n",
       " '1/': 'CARDINAL',\n",
       " '2/': 'CARDINAL',\n",
       " '@MountainAmerica': 'LOC',\n",
       " 'Utah': 'GPE',\n",
       " 'last week': 'DATE',\n",
       " '@UtahsCUs': 'ORG',\n",
       " 'this week': 'DATE',\n",
       " 'DC': 'GPE',\n",
       " 'CFPB': 'ORG',\n",
       " 'LTC Partners': 'ORG',\n",
       " 'Portsmouth': 'GPE',\n",
       " 'Florida': 'GPE',\n",
       " 'Quincy': 'GPE',\n",
       " 'Salvation Army': 'ORG',\n",
       " 'Wheels': 'ORG',\n",
       " 'Iowa': 'GPE',\n",
       " 'Midwest': 'LOC',\n",
       " '#DemDebate': 'PERSON',\n",
       " 'Trump &amp': 'ORG',\n",
       " 'Arizona': 'GPE',\n",
       " 'https://t.co/LeIvFnC68': 'PERSON',\n",
       " 'this afternoon': 'TIME',\n",
       " 'the day': 'DATE',\n",
       " 'Crawford &amp': 'ORG',\n",
       " 'Breazeale Drug Co.': 'ORG',\n",
       " 'Lincolnton': 'GPE',\n",
       " 'Georgians': 'NORP',\n",
       " '45': 'MONEY',\n",
       " 'Iowans': 'NORP',\n",
       " 'the Consumer Product Safety Commission': 'ORG',\n",
       " 'GarysYearinReview': 'MONEY',\n",
       " '5 DAYS': 'DATE',\n",
       " 'Minnesota': 'GPE',\n",
       " 'Team Tina': 'PERSON',\n",
       " 'FCC': 'ORG',\n",
       " 'three years': 'DATE',\n",
       " 'the last 10 years': 'DATE',\n",
       " 'w/': 'LANGUAGE',\n",
       " '35': 'CARDINAL',\n",
       " '#': 'CARDINAL',\n",
       " 'https://t.co/XkeCuj40UB': 'ORG',\n",
       " 'Ohioans': 'NORP',\n",
       " '2019': 'DATE',\n",
       " 'Ohio': 'GPE',\n",
       " '2020': 'DATE',\n",
       " 'Montana': 'GPE',\n",
       " 'any given day': 'DATE',\n",
       " 'the Trump Administration’s': 'ORG',\n",
       " 'Guntersville': 'GPE',\n",
       " 'Monday, January 27\\xa0': 'DATE',\n",
       " 'the House Managers’': 'ORG',\n",
       " 'Constance Baker Motley': 'PERSON',\n",
       " 'the Civil Rights Movement': 'ORG',\n",
       " '1st': 'ORDINAL',\n",
       " 'African American': 'NORP',\n",
       " 'NY': 'GPE',\n",
       " 'State Senate': 'ORG',\n",
       " 'BlackHistoryMonth': 'MONEY',\n",
       " 'https://t.co/j6JMbqWaGI': 'ORG',\n",
       " 'Main Street': 'FAC',\n",
       " 'Iowan': 'GPE',\n",
       " 'New Jersey': 'GPE',\n",
       " 'every day': 'DATE',\n",
       " 'NationalNursesWeek': 'MONEY',\n",
       " 'Mark': 'PERSON',\n",
       " 'Facebook Live Town Hall': 'ORG',\n",
       " '@SenJohnBarrasso, @JimInhofe': 'ORG',\n",
       " '@SenToomey': 'ORG',\n",
       " '@SenJohnKennedy': 'ORG',\n",
       " '@SenMikeLee': 'PERSON',\n",
       " '@SenBillCassidy': 'ORG',\n",
       " 'the Tenth Circuit’s': 'ORG',\n",
       " 'https://t.co/yh7yYvkyqL': 'CARDINAL',\n",
       " 'Rubio': 'PERSON',\n",
       " 'SmallBiz Radio Show': 'EVENT',\n",
       " '@barrymoltz': 'ORG',\n",
       " 'February 6, 2020': 'DATE',\n",
       " 'StateOfHealthCare': 'ORG',\n",
       " '@POTUS': 'ORG',\n",
       " 'Medicare &amp': 'ORG',\n",
       " 'Medicaid': 'ORG',\n",
       " 'a week': 'DATE',\n",
       " 'One year ago today': 'DATE',\n",
       " '90%': 'PERCENT',\n",
       " '#EndGunViolence': 'PERSON',\n",
       " '59%': 'PERCENT',\n",
       " 'Michael Atkinson': 'PERSON',\n",
       " 'Inspector General': 'ORG',\n",
       " 'the Intelligence Community': 'ORG',\n",
       " 'Tim Melia': 'PERSON',\n",
       " '#MakeTheMasks': 'PERSON',\n",
       " 'Trump Admin': 'ORG',\n",
       " '$45 billion': 'MONEY',\n",
       " 'BOA': 'ORG',\n",
       " '400k': 'MONEY',\n",
       " 'Delawareans': 'NORP',\n",
       " 'the last two days': 'DATE',\n",
       " 'Adam Schiff': 'PERSON',\n",
       " 'North Carolinians': 'NORP',\n",
       " 'NationalTeachersDay': 'PERSON',\n",
       " 'a few moments': 'TIME',\n",
       " 'Facebook': 'ORG',\n",
       " 'Holocaust': 'EVENT',\n",
       " 'the Iraq War': 'EVENT',\n",
       " 'NAFTA': 'LOC',\n",
       " 'Tony Fauci': 'PERSON',\n",
       " 'Donald Trump’s': 'PERSON',\n",
       " 'Colorado': 'GPE',\n",
       " '@IRSnews': 'ORG',\n",
       " 'April 15': 'DATE',\n",
       " 'Uni': 'ORG',\n",
       " '157': 'CARDINAL',\n",
       " 'SoDakSt 4/0': 'CARDINAL',\n",
       " 'Tribal': 'ORG',\n",
       " 'USPS': 'ORG',\n",
       " 'https://t.co/SlsRFLp7Ud': 'PERSON',\n",
       " 'the Middle East': 'LOC',\n",
       " '1️⃣': 'CARDINAL',\n",
       " '0️⃣': 'CARDINAL',\n",
       " 'fed': 'ORG',\n",
       " 'a quarter': 'CARDINAL',\n",
       " 'a million $$': 'MONEY',\n",
       " 'mascots &amp': 'ORG',\n",
       " 'at least April 30th': 'DATE',\n",
       " 'march': 'DATE',\n",
       " 'Martin Luther King Jr.': 'PERSON',\n",
       " 'Farmers': 'ORG',\n",
       " '$9.5 billion': 'MONEY',\n",
       " '20 weeks': 'DATE',\n",
       " '8th': 'ORDINAL',\n",
       " 'Holy Cross': 'ORG',\n",
       " 'Indianapolis': 'GPE',\n",
       " '@AARPOregon': 'ORG',\n",
       " 'Andrus Award': 'WORK_OF_ART',\n",
       " 'Geneva Craig': 'PERSON',\n",
       " 'MLK': 'PERSON',\n",
       " 'Medford': 'GPE',\n",
       " 'Craig': 'PERSON',\n",
       " 'This Holocaust Remembrance Day': 'EVENT',\n",
       " 'WeRemember': 'PERSON',\n",
       " 'Fox News @seanhannity': 'ORG',\n",
       " 'just a few minutes': 'TIME',\n",
       " '@librarycongress': 'ORG',\n",
       " '2018FarmBill': 'MONEY',\n",
       " 'Navy': 'ORG',\n",
       " '#AmericaStrong': 'PERSON',\n",
       " '23,786': 'CARDINAL',\n",
       " 'North Carolina Businesses Protected to Date': 'GPE',\n",
       " '$5.72 Billion': 'MONEY',\n",
       " 'Assistance': 'GPE',\n",
       " 'North Carolina Small Businesses': 'GPE',\n",
       " 'Tacoma': 'GPE',\n",
       " 'Vermont': 'GPE',\n",
       " 'third': 'ORDINAL',\n",
       " 'Congressional Delegation Town Hall': 'ORG',\n",
       " '5 p.m.': 'TIME',\n",
       " '#vtpoli': 'PERSON',\n",
       " 'ICYMI': 'ORG',\n",
       " 'CommonGoodCapitalism': 'MONEY',\n",
       " '@AmericanAffrs': 'PRODUCT',\n",
       " 'an additional $16 million': 'MONEY',\n",
       " 'Washington Democrats': 'NORP',\n",
       " 'Joan Trumpauer Mulholland': 'PERSON',\n",
       " 'the Freedom Riders': 'EVENT',\n",
       " 'Joan': 'PERSON',\n",
       " 'the Trump Administration': 'ORG',\n",
       " 'https://t.co/ZtlhbXABHW': 'CARDINAL',\n",
       " 'More than 17,000 #': 'MONEY',\n",
       " 'Kentucky': 'GPE',\n",
       " 'over $3.3 billion': 'MONEY',\n",
       " 'last week’s': 'DATE',\n",
       " 'One': 'CARDINAL',\n",
       " 'w/ China &amp': 'ORG',\n",
       " 'sick days': 'DATE',\n",
       " 'Sandia': 'GPE',\n",
       " 'New Mexico': 'GPE',\n",
       " 'New Mexicans': 'NORP',\n",
       " 'North Carolina': 'GPE',\n",
       " 'a decade': 'DATE',\n",
       " '#Soleimani': 'PERSON',\n",
       " 'Iraq': 'GPE',\n",
       " 'Anti-Trump': 'LAW',\n",
       " '@potus': 'ORG',\n",
       " 'Iran': 'GPE',\n",
       " 'IRGC': 'ORG',\n",
       " 'CT': 'PERSON',\n",
       " 'BofA': 'ORG',\n",
       " 'Wells Fargo': 'ORG',\n",
       " '10 years ago': 'DATE',\n",
       " 'AZ farmers &amp': 'ORG',\n",
       " 'COVID-19': 'ORDINAL',\n",
       " 'Administration': 'ORG',\n",
       " '@GovInslee': 'ORG',\n",
       " 'WA': 'ORG',\n",
       " 'gov’ts': 'PRODUCT',\n",
       " '9': 'CARDINAL',\n",
       " 'the State of the Union': 'GPE',\n",
       " 'Barbara Barrett': 'PERSON',\n",
       " 'the Air Force': 'ORG',\n",
       " 'the Idaho Small Business of the': 'ORG',\n",
       " 'December 2019': 'DATE',\n",
       " 'R &amp': 'ORG',\n",
       " 'H Machine': 'ORG',\n",
       " 'Caldwell': 'ORG',\n",
       " 'next week': 'DATE',\n",
       " 'Dems': 'NORP',\n",
       " 'Vázquez': 'PERSON',\n",
       " 'Puerto Rico': 'GPE',\n",
       " 'July 15': 'DATE',\n",
       " 'Iranian': 'NORP',\n",
       " 'the White House': 'ORG',\n",
       " 'SD': 'GPE',\n",
       " 'South Haven': 'GPE',\n",
       " 'the Green New Deal': 'ORG',\n",
       " 'the next Congress': 'DATE',\n",
       " 'FAA': 'ORG',\n",
       " '@FAANews': 'ORG',\n",
       " 'the @tristateairport Air Traffic Control tower': 'ORG',\n",
       " 'Huntington': 'GPE',\n",
       " '24hr': 'DATE',\n",
       " 'Economic Impact Payments': 'ORG',\n",
       " 'up to $1,200': 'MONEY',\n",
       " 'SS': 'ORG',\n",
       " 'Rural electric co-ops': 'ORG',\n",
       " '@SBAGov': 'PRODUCT',\n",
       " 'PaycheckProtectionProgram': 'MONEY',\n",
       " 'more than $41 million': 'MONEY',\n",
       " '$20.5 million': 'MONEY',\n",
       " 'https://t.co/VXZchpdc8Y': 'PERSON',\n",
       " 'NationalNursesDay': 'MONEY',\n",
       " 'This year': 'DATE',\n",
       " 'Marco Rubio': 'PERSON',\n",
       " '@jguaido': 'ORG',\n",
       " 'Maduro': 'ORG',\n",
       " 'GNB': 'MONEY',\n",
       " '#WarrenSelfie 100,000': 'MONEY',\n",
       " 'Manchester': 'GPE',\n",
       " '100,000': 'CARDINAL',\n",
       " 'The Family Research Council': 'ORG',\n",
       " '100%': 'PERCENT',\n",
       " \"the FRC Action's\": 'ORG',\n",
       " '1/3': 'CARDINAL',\n",
       " 'Joshua 1:9': 'LAW',\n",
       " 'this Saturday': 'DATE',\n",
       " '5': 'CARDINAL',\n",
       " 'ten': 'CARDINAL',\n",
       " 'Laura Dove': 'PERSON',\n",
       " 'United States': 'GPE',\n",
       " 'Laura': 'PERSON',\n",
       " 'Last night': 'TIME',\n",
       " 'The Main Street Emergency Grant Program': 'FAC',\n",
       " '$195 billion': 'MONEY',\n",
       " 'COVID': 'ORG',\n",
       " '@RepLloydDoggett &amp': 'ORG',\n",
       " 'more than 77,000': 'CARDINAL',\n",
       " 'Last month': 'DATE',\n",
       " 'Jason Soto': 'PERSON',\n",
       " 'year': 'DATE',\n",
       " '#SOTU https://t.co/8tCOT4M53r': 'PERSON',\n",
       " '1787': 'CARDINAL',\n",
       " '$300 million': 'MONEY',\n",
       " 'The Commerce Department': 'ORG',\n",
       " 'Illinois': 'GPE',\n",
       " 'https://t.co/CxUQi8h6i4': 'LAW',\n",
       " '@HarkinsTheatres @lifetimefitness': 'ORG',\n",
       " 'Mar-a': 'DATE',\n",
       " 'Mount Pleasant': 'GPE',\n",
       " 'just 4 days': 'DATE',\n",
       " 'Gates': 'ORG',\n",
       " 'the Innovation Fairy': 'ORG',\n",
       " 'over $600 Billion': 'MONEY',\n",
       " 'Tomorrow': 'DATE',\n",
       " 'night': 'TIME',\n",
       " '👇🏾 https://t.co/9C7Q4CDakq': 'PRODUCT',\n",
       " 'the past week': 'DATE',\n",
       " '✅ Historic': 'ORG',\n",
       " '✅': 'ORG',\n",
       " 'four': 'CARDINAL',\n",
       " 'republican': 'NORP',\n",
       " 'the Ohio Fraternal Order of Police': 'ORG',\n",
       " 'the Affordable Care Act &amp': 'LAW',\n",
       " 'StateofHealthCare': 'MONEY',\n",
       " '540,000': 'MONEY',\n",
       " '10,000': 'CARDINAL',\n",
       " 'Every day': 'DATE',\n",
       " 'Yesterday': 'DATE',\n",
       " 'day 1': 'DATE',\n",
       " 'NationalDayofPrayer': 'MONEY',\n",
       " '#MLKDay https://t.co/uo6r5pFh5f': 'PERSON',\n",
       " 'Clemson': 'PERSON',\n",
       " 'Tiger Country': 'ORG',\n",
       " '#LetsGeaux': 'PERSON',\n",
       " 'al-Rimi': 'NORP',\n",
       " 'al-Qaeda': 'ORG',\n",
       " 'LOVELOUD': 'MONEY',\n",
       " 'Wyoming': 'GPE',\n",
       " '1PM': 'TIME',\n",
       " 'IRS': 'ORG',\n",
       " 'https://t.co/v1I9N5wiR6': 'PRODUCT',\n",
       " '@SenatorShaheen': 'ORG',\n",
       " 'POTUS &amp': 'ORG',\n",
       " 'COBRA': 'ORG',\n",
       " '#NationalAgDay': 'MONEY',\n",
       " '43,000 million-plus-dollar': 'MONEY',\n",
       " '$1.6 million': 'MONEY',\n",
       " 'para una sociedad sana': 'ORG',\n",
       " 'Hoy al conmemorar': 'PERSON',\n",
       " 'Nicaragua': 'GPE',\n",
       " 'honramos el trabajo y la valentía de aquellos': 'PERSON',\n",
       " 'independientes que arriesgan sus vidas en': 'PERSON',\n",
       " 'Berkshires': 'GPE',\n",
       " 'NH': 'GPE',\n",
       " '15': 'MONEY',\n",
       " 'FEMA': 'ORG',\n",
       " 'Chinese Communist': 'NORP',\n",
       " 'Act': 'LAW',\n",
       " 'Hollywood': 'GPE',\n",
       " 'Saturday': 'DATE',\n",
       " 'Filipino-American': 'NORP',\n",
       " 'Chicago': 'GPE',\n",
       " 'Friday': 'DATE',\n",
       " 'Duterte': 'GPE',\n",
       " 'Filipino': 'NORP',\n",
       " 'Iranians': 'NORP',\n",
       " 'Iranian-Americans': 'NORP',\n",
       " 'Hampton': 'GPE',\n",
       " 'the last few days': 'DATE',\n",
       " 'OregonWay': 'ORG',\n",
       " 'cooks &amp': 'ORG',\n",
       " 'Las Vegas': 'GPE',\n",
       " 'CNNTownHall': 'WORK_OF_ART',\n",
       " 'early in': 'DATE',\n",
       " 'Canyon County': 'GPE',\n",
       " 'Lincoln Day': 'DATE',\n",
       " 'Nampa': 'GPE',\n",
       " 'about one': 'CARDINAL',\n",
       " 'thousands': 'CARDINAL',\n",
       " 'Floridians': 'NORP',\n",
       " 'FloridiansStuckAbroad': 'MONEY',\n",
       " 'Fort Wayne': 'PERSON',\n",
       " 'FDA': 'ORG',\n",
       " 'Stephen Hahn': 'PERSON',\n",
       " 'https://t.co/NCIa7clvI5': 'PERSON',\n",
       " 'The Chinese Communist Party': 'ORG',\n",
       " 'a long week': 'DATE',\n",
       " 'WealthTax': 'MONEY',\n",
       " 'universal free public college': 'ORG',\n",
       " 'up to 100%': 'PERCENT',\n",
       " '90 days': 'DATE',\n",
       " 'DHS': 'ORG',\n",
       " 'Clearview': 'ORG',\n",
       " 'Healthy Michigan': 'PERSON',\n",
       " 'nearly 700,000': 'CARDINAL',\n",
       " 'Michiganders': 'LOC',\n",
       " '🏥': 'ORG',\n",
       " 'Social Security': 'ORG',\n",
       " '6 million': 'CARDINAL',\n",
       " 'Jews': 'NORP',\n",
       " 'Nazi': 'NORP',\n",
       " '@AIPAC': 'ORG',\n",
       " 'Israel': 'GPE',\n",
       " 'the Congressional Economic Task Force': 'ORG',\n",
       " 'https://t.co/EWj3DjN3QK': 'PERSON',\n",
       " 'the night': 'TIME',\n",
       " 'Lifeline': 'ORG',\n",
       " 'https://t.co/4lsPQByHli': 'ORG',\n",
       " 'SGT Daniel A. Arziga': 'ORG',\n",
       " 'Charge': 'ORG',\n",
       " 'Vehicle Control Officer': 'ORG',\n",
       " 'Detachment Delta Company': 'ORG',\n",
       " '4th Law Enforcement Battalion': 'ORG',\n",
       " 'JBER': 'ORG',\n",
       " 'the Year': 'DATE',\n",
       " '@ASYMCA': 'ORG',\n",
       " 'the Supreme Court': 'ORG',\n",
       " 'DACA': 'ORG',\n",
       " '@ChuckGrassley': 'ORG',\n",
       " '#FlattenTheCurve': 'PERSON',\n",
       " 'heath': 'PERSON',\n",
       " 'South Carolina': 'GPE',\n",
       " 'Ted Cruz Shreds': 'PERSON',\n",
       " 'Bernie': 'PERSON',\n",
       " '📺': 'ORG',\n",
       " 'Afghanistan': 'GPE',\n",
       " 'Homeland Security Committee': 'ORG',\n",
       " 'Baltimore City Health': 'FAC',\n",
       " '@DrTishCommish': 'PERSON',\n",
       " 'Thursday': 'DATE',\n",
       " '3:30pm': 'TIME',\n",
       " '2:05 PM Central': 'TIME',\n",
       " 'circus House': 'ORG',\n",
       " 'Leadville': 'GPE',\n",
       " '@RepDannyDavis': 'ORG',\n",
       " 'Trauma Act &amp': 'ORG',\n",
       " 'World Health Organization: Surgical': 'ORG',\n",
       " 'tens of thousands': 'CARDINAL',\n",
       " 'billions of dollars': 'MONEY',\n",
       " 'Kick Big Oil': 'ORG',\n",
       " 'authorized &amp': 'ORG',\n",
       " 'July 15th': 'DATE',\n",
       " '3.5B': 'MONEY',\n",
       " 'Vietnam': 'GPE',\n",
       " '@DHSgov': 'ORG',\n",
       " 'Utahns': 'ORG',\n",
       " '@SouthernCompany': 'ORG',\n",
       " 'HBCU': 'ORG',\n",
       " 'https://t.co/PpBhUFa9xr': 'ORG',\n",
       " 'Wednesday': 'DATE',\n",
       " 'morning': 'TIME',\n",
       " '@RussVought45': 'ORG',\n",
       " 'the Office of Management and Budget': 'ORG',\n",
       " 'Russ': 'PERSON',\n",
       " 'years': 'DATE',\n",
       " '1,200': 'MONEY',\n",
       " 'per adult &amp': 'ORG',\n",
       " '500': 'CARDINAL',\n",
       " '377B': 'MONEY',\n",
       " 'Kim Jong Un': 'PERSON',\n",
       " 'the Department of Education': 'ORG',\n",
       " 'the end of September': 'DATE',\n",
       " 'DECADES': 'DATE',\n",
       " 'Arkansas Valley Conduit': 'ORG',\n",
       " 'Kennedy': 'PERSON',\n",
       " '50,000': 'CARDINAL',\n",
       " 'Coloradans': 'NORP',\n",
       " '$28 million': 'MONEY',\n",
       " 'Purell': 'ORG',\n",
       " '#coronavirus': 'MONEY',\n",
       " 'w/ @USTradeRep': 'PERSON',\n",
       " 'Asian Americans': 'NORP',\n",
       " 'the Asian American': 'NORP',\n",
       " 'Pacific Islander': 'LOC',\n",
       " 'more than 34 million': 'CARDINAL',\n",
       " 'McConnell': 'ORG',\n",
       " '#Kentucky\\n': 'MONEY',\n",
       " '1905-1982': 'DATE',\n",
       " 'Russian-American': 'NORP',\n",
       " 'un': 'ORG',\n",
       " 'Tucson': 'GPE',\n",
       " 'Matthew Fosdick': 'PERSON',\n",
       " 'Empire High School': 'ORG',\n",
       " 'just 300': 'CARDINAL',\n",
       " 'Matthew': 'PERSON',\n",
       " 'The U.S. Constitution': 'LAW',\n",
       " 'nearly 1/2': 'CARDINAL',\n",
       " '240 days': 'DATE',\n",
       " 'Two years ago': 'DATE',\n",
       " 'Reagan': 'PERSON',\n",
       " '40th': 'ORDINAL',\n",
       " 'ValentinesDay': 'PERSON',\n",
       " 'Walter Reed': 'PERSON',\n",
       " '6mo': 'DATE',\n",
       " 'Grady Hospital': 'ORG',\n",
       " '50 years': 'DATE',\n",
       " 'the past three yrs': 'DATE',\n",
       " '#SOTU': 'PERSON',\n",
       " 'Schiff &amp': 'ORG',\n",
       " 'Flentie': 'PERSON',\n",
       " 'Lewistown Jr. High School': 'ORG',\n",
       " 'modern day': 'DATE',\n",
       " 'Cal Cunningham': 'ORG',\n",
       " 'Erica Smith': 'PERSON',\n",
       " 'this fall': 'DATE',\n",
       " 'Elizabeth Warren': 'PERSON',\n",
       " 'PresidentsDay': 'MONEY',\n",
       " 'Steve Bannon': 'PERSON',\n",
       " '@JasonMillerinDC': 'NORP',\n",
       " 'a minute': 'TIME',\n",
       " 'Torrin Howard': 'PERSON',\n",
       " 'Waterbury': 'GPE',\n",
       " 'https://t.co/BErnvhMZNK': 'ORG',\n",
       " 'several days': 'DATE',\n",
       " 'Democrats &amp': 'ORG',\n",
       " 'decades': 'DATE',\n",
       " \"por el resultado q mejor promoviera el '\": 'PRODUCT',\n",
       " \"bien común'\": 'ORG',\n",
       " 'La': 'DATE',\n",
       " 'basada en un juicio político partidista': 'FAC',\n",
       " 'por voto limitado y sin amplio apoyo público': 'PRODUCT',\n",
       " '➗': 'ORG',\n",
       " 'amargura y amenazaría': 'PERSON',\n",
       " 'nuestro país por décadas': 'PRODUCT',\n",
       " 'Mexico City': 'GPE',\n",
       " '#Fight4Her': 'PERSON',\n",
       " 'https://t.co/ZlAdwALrMz': 'ORG',\n",
       " 'the President &amp': 'ORG',\n",
       " '#FITN #': 'MONEY',\n",
       " 'Today marks': 'MONEY',\n",
       " '50th': 'ORDINAL',\n",
       " '9:30am EST': 'TIME',\n",
       " 'HSGAC': 'ORG',\n",
       " '@RepAdamSchiff': 'ORG',\n",
       " '844': 'CARDINAL',\n",
       " 'ranchers &amp': 'ORG',\n",
       " 'the State Department': 'ORG',\n",
       " 'just the past week': 'DATE',\n",
       " 'March': 'DATE',\n",
       " 'PaycheckProtectionProgram &amp': 'ORG',\n",
       " 'Health Care Enhancement Act': 'LAW',\n",
       " '@realdonaldtrump': 'ORG',\n",
       " 'James': 'PERSON',\n",
       " 'Texans': 'NORP',\n",
       " 'Trey Trainor': 'PERSON',\n",
       " 'FEC': 'ORG',\n",
       " '2003': 'DATE',\n",
       " 'the Voting Rights Act': 'LAW',\n",
       " 'St. Louis County': 'GPE',\n",
       " 'one month': 'DATE',\n",
       " 'bank &amp': 'ORG',\n",
       " '3 lost days': 'DATE',\n",
       " 'Democrat': 'NORP',\n",
       " 'Josh Hawley': 'PERSON',\n",
       " 'Hunter Biden': 'PERSON',\n",
       " 'Burisma': 'LOC',\n",
       " '3:10 p.m.': 'TIME',\n",
       " '1979': 'DATE',\n",
       " 'Ed Markey': 'PERSON',\n",
       " 'https://t.co/OGv6WMR7OA': 'PERSON',\n",
       " 'hundreds of millions': 'CARDINAL',\n",
       " '@chrislhayes': 'ORG',\n",
       " '@MSNBC': 'ORG',\n",
       " '8pm EST': 'TIME',\n",
       " 'Australia': 'GPE',\n",
       " 'Chris Murphy': 'PERSON',\n",
       " 'USMCA': 'ORG',\n",
       " '2nd': 'ORDINAL',\n",
       " '@SenatorRisch': 'ORG',\n",
       " '@sendavidperdue': 'ORG',\n",
       " '@SenJoniErnst &amp': 'ORG',\n",
       " '@SenatorFischer': 'ORG',\n",
       " 'Portsmouth Naval Shipyard': 'ORG',\n",
       " 'N95': 'PRODUCT',\n",
       " '@RepAnnieKuster': 'GPE',\n",
       " '@RepChrisPappas': 'ORG',\n",
       " 'Louisville': 'GPE',\n",
       " '| Kentucky': 'ORG',\n",
       " 'the end of this month': 'DATE',\n",
       " '500B': 'MONEY',\n",
       " 'One year from today': 'DATE',\n",
       " 'her first full day': 'DATE',\n",
       " '#WeWantWitnesses': 'PERSON',\n",
       " 'WeWantDocuments': 'MONEY',\n",
       " 'Mulvaney': 'PERSON',\n",
       " 'Qassim Suleimani': 'PERSON',\n",
       " 'over a decade': 'DATE',\n",
       " '94%': 'PERCENT',\n",
       " 'Arizonans': 'NORP',\n",
       " 'Congressional': 'ORG',\n",
       " 'the Second Amendment': 'LAW',\n",
       " 'the TRACED Act': 'LAW',\n",
       " 'EMS': 'ORG',\n",
       " 'Fire Training Academies': 'ORG',\n",
       " 'https://t.co/herZhwBoSp': 'CARDINAL',\n",
       " 'Erin': 'PERSON',\n",
       " 'evening': 'TIME',\n",
       " 'the White House Team': 'ORG',\n",
       " '11:30am': 'CARDINAL',\n",
       " '3rd': 'ORDINAL',\n",
       " '#SOTU2020': 'PERSON',\n",
       " 'NEPA': 'ORG',\n",
       " 'Kyle': 'PERSON',\n",
       " 'New York': 'GPE',\n",
       " 'Last week': 'DATE',\n",
       " 'Pitiful &amp': 'ORG',\n",
       " 'Kentuckian': 'NORP',\n",
       " 'Justin Walker': 'PERSON',\n",
       " 'the U.S. Senate Judiciary Committee': 'ORG',\n",
       " 'the U.S. Court of Appeals': 'ORG',\n",
       " 'the D.C. Circuit': 'ORG',\n",
       " '34': 'CARDINAL',\n",
       " '#CincodeMayo': 'PERSON',\n",
       " 'Mexican-Americans': 'NORP',\n",
       " '@SenatorMenendez': 'ORG',\n",
       " 'Cinco de Mayo': 'ORG',\n",
       " 'Raleigh Family &amp': 'ORG',\n",
       " 'Si Leis': 'PERSON',\n",
       " 'Madi': 'PERSON',\n",
       " 'Raleigh': 'GPE',\n",
       " 'Phoenix': 'GPE',\n",
       " 'We Are Going to Deliver': 'WORK_OF_ART',\n",
       " 'more than $39M': 'MONEY',\n",
       " 'Puerto Rican': 'NORP',\n",
       " 'Vieques': 'GPE',\n",
       " 'Hurricanes Maria &amp': 'ORG',\n",
       " 'Irma': 'PERSON',\n",
       " '@AARPWV': 'ORG',\n",
       " '#Mobile': 'MONEY',\n",
       " '@USACEHQ': 'ORG',\n",
       " '$274.3 million': 'MONEY',\n",
       " 'the last decade': 'DATE',\n",
       " 'the Port of Mobile': 'ORG',\n",
       " '1/4': 'CARDINAL',\n",
       " 'day 9': 'DATE',\n",
       " 'about 4 hours': 'TIME',\n",
       " 'the @IL_Natl_Guard 2nd Battalion': 'ORG',\n",
       " '130th Infantry Regiment': 'ORG',\n",
       " 'Marion': 'GPE',\n",
       " 'West Frankfort': 'GPE',\n",
       " 'Mount Vernon': 'GPE',\n",
       " 'Effingham': 'GPE',\n",
       " 'Litchfield': 'GPE',\n",
       " '10-month': 'DATE',\n",
       " 'service &amp': 'ORG',\n",
       " 'Earlier this week': 'DATE',\n",
       " 'Braun': 'ORG',\n",
       " 'my days': 'DATE',\n",
       " 'RI': 'ORG',\n",
       " 'General &amp': 'ORG',\n",
       " ...}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, the accuracy isn't bad, but it's far from perfect. A `GPE` is a geopolitical designation, like a city or a state, and those seem largley to be classified correctly. The results for `ORG` and `PERSON` are spottier. It flags certain hashtags as `PERSON`s and \"PPP\" as an `ORG`, which in thise case, it's probably not. Also, the entity models picks out some phrases in Spanish which do not appear to refer to named entities at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "See if you can create a function that calculates the number of times each unique entity appears in our dataset, classified by entity label. The result would be similar to what we did with `pos_dict` above. The function should accpet our list of `Document` objects and returns a dictionary such that `ent_dict['PERSON']` shows all the unique `PERSON` entities and their frequency in the collection, and so that we can write `ent_dict['PERSON'].most_common(N)` to see the top N entities of that type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "```\n",
    "def count_ents(docs):\n",
    "    ent_dict = defaultdict(Counter) # Initialize our nest dictionary with a Counter\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents: # Loop over doc.ents to get the entities, not over doc, which returns the tokens\n",
    "            label = ent.label_ # The descriptive label/category\n",
    "            text = ent.text # The string representation of the entity; note that entities aren't lemmatized\n",
    "            ent_dict[label][text] += 1 # Increment the Counter associated with that label\n",
    "    return ent_dict\n",
    "```\n",
    "\n",
    "If we run the above to find the top 20 persons, we can observe a few things:\n",
    "- \"#COVID19\" is mistakenly labeled as a PERSON.\n",
    "- Named entities aren't lemmatized, so names like \"Bolton\" and \"John Bolton\" show up as separate entities.\n",
    "\n",
    "You can update individual entities to correct their classification, but this has to be done at the document level, so it would require writing code to update each document where the entity appears. You can also train your own entity model, but that requires having a pre-tagged dataset to train the model on. See the [spaCy docs](https://spacy.io/usage/linguistic-features#named-entities) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Trump', 255),\n",
       " ('#COVID19', 88),\n",
       " ('Donald Trump', 69),\n",
       " ('Rubio', 61),\n",
       " ('McConnell', 60),\n",
       " ('#DemDebate', 48),\n",
       " ('@realDonaldTrump', 46),\n",
       " ('Mitch McConnell', 36),\n",
       " ('SOTU', 24),\n",
       " ('Hawley', 23),\n",
       " ('John Bolton', 23),\n",
       " ('#SOTU', 23),\n",
       " ('Bernie Sanders', 21),\n",
       " ('Obama', 20),\n",
       " ('Pelosi', 19),\n",
       " ('#ICYMI', 16),\n",
       " ('Bolton', 16),\n",
       " ('Marco Rubio', 16),\n",
       " ('Nevadans', 16),\n",
       " ('Joe Biden', 15)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_dict = count_ents(docs)\n",
    "ent_dict['PERSON'].most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word and document similarity\n",
    "\n",
    "A more ambitious form of semantic analysis seeks quantitatively to represent the _meanings_ of words based on their relative similarity to other words. As you might imagine, this task is fraught with difficulty, since meaning in natural languages is so highly contextual. The words we speak or write represent just the tip of the iceberg of what we mean -- a fact attested to by how frequently a community of human speakers can disagree about the meaning of the utterances their members produce. Certain words stand out as flashpoints of controversy: think about the range of meanings people attribute to a word like _racism_ or _safety_ or _science_. But in general, this indeterminancy affects all human language. Beneath the written text or spoken utterance lies a huge weight of personal experience and collective history.\n",
    "\n",
    "NLP algorithms generally represent \"meaning\" in terms of a much narrower version of context: namely, the collocation of linguistic features across a corpous of texts. These algorithms tend either to apply sophisticated statistical techniques or -- more recently -- to make use of neural networks. But the basic premise is that \"similar\" or \"related\" words tend to occur more frequently together than dismilar or unrelated words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy's models produce both `Token` and `Document` **vectors**, which are derived from a technique called [word2vec](https://en.wikipedia.org/wiki/Word2vec). The vectors by themselves are not terribly informative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.4808e-01, -2.8244e-02, -1.5513e-03, -4.4549e-01, -6.6233e-01,\n",
       "        3.6924e-01,  2.1900e-01, -4.7495e-01,  5.7171e-02,  1.1867e+00,\n",
       "       -9.2176e-02, -8.6183e-02,  7.4074e-02, -2.0607e-01, -1.9715e-01,\n",
       "        5.9195e-02,  1.1783e-01, -1.0074e-01,  1.5625e-01,  4.0891e-01,\n",
       "        2.4840e-04, -5.1546e-02,  3.1825e-01, -1.9916e-01,  1.4442e-02,\n",
       "        1.2510e-02,  7.0528e-02, -2.3162e-01,  2.3331e-01, -4.0152e-01,\n",
       "        3.3923e-01,  9.7194e-02, -1.5903e-01,  4.0869e-01,  1.0443e-01,\n",
       "        1.0303e-01, -2.5341e-01,  2.0457e-02,  3.0006e-01,  1.5161e-01,\n",
       "       -1.7320e-01,  1.0995e-01, -3.2460e-01, -1.6000e-01,  1.1650e-01,\n",
       "        2.9631e-01,  2.5647e-02,  7.6841e-01, -1.3053e-01, -3.8559e-01,\n",
       "        9.0185e-02,  3.1918e-01,  4.1669e-01, -2.8903e-01, -2.6488e-01,\n",
       "       -4.8447e-02,  2.4093e-01,  2.9484e-01, -5.0827e-01, -3.1536e-01,\n",
       "       -4.2685e-01, -3.3898e-01, -2.1983e-01, -2.6768e-01, -1.0850e-01,\n",
       "       -1.7611e-01,  3.3296e-01, -3.0399e-01,  2.9700e-01, -5.3963e-02,\n",
       "        3.1203e-01, -1.5511e-01, -3.6819e-01,  3.9183e-01, -2.6244e-01,\n",
       "       -4.3520e-02,  6.3582e-01,  2.4318e-02, -3.4361e-02,  7.0844e-01,\n",
       "        3.8460e-01,  2.8425e-01,  7.7951e-02,  2.3984e-01, -8.1576e-02,\n",
       "       -9.9450e-02,  5.7592e-01, -1.0098e-01, -1.4186e-01,  1.3650e-01,\n",
       "        1.6196e-01, -3.2622e-01, -5.4137e-01,  1.0115e+00,  6.4264e-01,\n",
       "       -2.0091e-02, -5.1532e-01, -6.4280e-01, -2.6611e-03,  3.5214e-01,\n",
       "       -3.1074e-01, -3.8420e-01,  4.8236e-01, -1.6514e-01,  6.8948e-01,\n",
       "       -3.4703e-01,  3.3116e-01, -3.6747e-01, -3.1169e-01, -8.7990e-02,\n",
       "       -3.1231e-01, -4.7985e-01,  4.3958e-01,  2.6613e-02, -1.9160e-01,\n",
       "        6.7646e-02, -2.1863e-02, -1.7501e-01,  4.3201e-01, -2.3605e-01,\n",
       "        3.0957e-01, -1.0382e-01, -2.4821e-02,  3.0351e-01,  3.6451e-01,\n",
       "        4.0542e-02,  7.2172e-02,  1.9603e-01, -5.4925e-01,  2.3661e-01,\n",
       "       -3.0754e-01,  4.5172e-01, -1.5854e-01, -2.5109e-01, -4.5159e-01,\n",
       "        2.7155e-01, -2.0309e-01, -3.0581e-03,  4.4840e-02,  6.1746e-02,\n",
       "       -1.1636e+00,  6.5584e-01,  5.3133e-01,  1.4806e-01, -1.7105e-01,\n",
       "        1.9976e-01,  4.4624e-03, -6.7372e-02, -1.1340e-01,  4.3267e-01,\n",
       "       -1.4532e-01,  5.0396e-02, -8.9211e-03, -3.1058e-01, -7.7238e-02,\n",
       "       -2.5300e-02,  4.6137e-01, -1.6162e-01,  3.1154e-01, -1.5355e-01,\n",
       "        2.5972e-01, -1.7490e-01, -1.9745e-01, -3.8769e-02, -2.2595e-01,\n",
       "       -3.2845e-02, -2.7819e-01, -3.3381e-01, -1.0537e-01, -5.0057e-01,\n",
       "       -1.0137e-01, -1.0781e-01,  4.8632e-01, -3.2592e-01,  4.2631e-01,\n",
       "       -2.4400e-01, -1.8532e-01, -1.8580e-01, -4.9346e-01, -2.7019e-01,\n",
       "        4.2283e-01, -3.5293e-01,  2.1400e-01,  2.2041e-01,  8.3586e-02,\n",
       "       -4.6200e-01,  3.7850e-01, -5.9818e-02, -1.5260e-01,  1.2653e-01,\n",
       "        3.0623e-01, -1.7778e-01, -1.5725e-01, -1.1630e-01,  3.5412e-01,\n",
       "       -3.0014e-01,  9.6444e-04, -3.1248e-01,  1.4135e-01,  3.5102e-01,\n",
       "        2.3681e-01, -5.8921e-01, -3.2036e-01,  3.5265e-02,  9.5263e-02,\n",
       "        1.4344e-02, -6.6267e-02,  3.8450e-01, -2.3133e-01, -4.8957e-01,\n",
       "       -4.3540e-01,  2.7288e-01,  9.6428e-03,  4.6485e-01, -5.6376e-02,\n",
       "        7.1252e-01,  1.1378e-01, -3.7955e-01,  6.3981e-01,  2.2585e-01,\n",
       "       -3.6530e-01, -4.3529e-01,  4.7649e-02,  3.1294e-01,  4.3872e-01,\n",
       "       -4.4774e-01, -4.7957e-02, -6.2946e-01,  2.1529e-01, -5.8139e-02,\n",
       "        2.1211e-02, -3.1313e-01, -5.8919e-01, -9.6554e-02, -1.7989e-01,\n",
       "        1.5908e-01, -6.9503e-02,  3.9633e-01, -1.5545e-02,  3.2559e-01,\n",
       "        3.0032e-02, -4.3776e-01, -7.0842e-02,  4.7318e-01,  3.2821e-01,\n",
       "        4.0274e-01,  2.4783e-01, -2.5787e-01,  1.9566e-01,  3.3179e-01,\n",
       "        3.2825e-01,  7.6107e-02, -5.4647e-01, -1.4823e-02, -1.8991e-01,\n",
       "       -1.8829e-01,  1.2603e-01, -4.1584e-02,  2.3014e-02, -3.3881e-01,\n",
       "       -3.4079e-02, -1.7421e-02, -5.2246e-01,  1.8848e-01, -2.3916e-01,\n",
       "       -3.5392e-01,  5.3217e-01,  3.0792e-01,  3.4795e-01,  1.4809e-01,\n",
       "       -4.6983e-01,  1.5812e-01, -7.2125e-02, -2.4025e-01,  7.8822e-03,\n",
       "        3.3578e-01,  4.3215e-01, -1.0158e-01, -8.2204e-02, -1.1184e-01,\n",
       "       -4.3991e-01, -1.1885e-01, -2.6322e-02, -7.1573e-02, -4.1064e-01,\n",
       "        1.1625e-01, -1.2499e-01,  1.9341e-01,  2.1922e-01,  9.3681e-02,\n",
       "        1.1598e-01, -7.1213e-02, -1.4393e-01,  3.7572e-01,  1.9229e-01,\n",
       "        1.0672e-01, -8.0200e-02,  2.9230e-01, -8.0471e-01,  7.1153e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0][0].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the vectors allow us to compare two tokens, using each token's built-in `.similarity` method. Tokens with a higher score are supposedly more similar. Let's take a few tokens in isolation to illustrate.\n",
    "\n",
    "Note that we are procssing the words first with the `nlp` function to create of each a spaCy `Document` consisting of a single token. We can't compare the similarity of unprocessed Python strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the model on individual words\n",
    "banana = nlp('banana')\n",
    "orange = nlp('orange')\n",
    "apple = nlp('apple')\n",
    "dog = nlp('dog')\n",
    "cat = nlp('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5629939782223348"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana.similarity(orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3288468980287254"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orange.similarity(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8016854705531046"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.similarity(dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems fairly reasonable to say that an orange is more similar to a banana than to a cat. I'm not sure why _cat_ and _dog_ appear so much more similar than _orange_ and _banana_, however.\n",
    "\n",
    "Also, the dubious logic of collocation appears in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5206573188004238"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.similarity(nlp('wolf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It stands to reason that the accuracy of the similarity model depends on the size and nature of the corpus used to train it. You can train your own word2vec models using other Python libraries (like [Gensim](https://radimrehurek.com/gensim/)) and import the results into spaCy; this approach might be particularly useful if you're working with a corpus of fairly specialized texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use word-vector similarity to analyze our dataset? \n",
    "\n",
    "One approach might be to look for the documents that are most similar to a given document. spaCy automatically assigns each `Document` a vector that represents the average of its word vectors, and this is used as the basis for the `Document.similarity` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066829319507925"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].similarity(docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kudos to @Toyota as its workforce prepares to return to work at TMMTX (San Antonio) on Monday, May 4.  Their \"Safe at Work Playbook\" is based on guidelines from the CDC, WHO, and OSHA, best practices developed by Toyota Working Groups, and local orders and other authorities."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Many @SocialSecurity beneficiaries were surprised by a recent @IRSnews rule that required a tax return to receive direct #COVID19 checks. My colleagues and I urged @USTreasury to waive this burdensome requirement.\n",
       " \n",
       "Tonight, this rule was reversed. https://t.co/Zx4jujdris"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since every token in a document has a vector, including punctuation, stopwords, and URL's, this score might be weighted by a lot of information we don't really care about. Can we make it cleaner by comparing only the vectors of \"content\" words?\n",
    "\n",
    "We can, but it requires a little bit of reverse-engineering. In what follows, we'll implement our own version of spaCy's `.similarity` method to measure the similarity between only the \"content\" words in our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vector of a document = average of the token vectors\n",
    "# We can use this to get the vector of the tokens minus stopwords, etc.\n",
    "def vectorize_without_stops(doc):\n",
    "    # Remember that our function remove_stops returns the list of content words in a given spaCy Document\n",
    "    # We're using numpy.array to create an array of the word vectors in that list\n",
    "    # Each word vector is already a numpy array, so we're creating a 2-dimensional array\n",
    "    vectors = np.array([token.vector for token in remove_stops(doc)])\n",
    "    # Some document vectors might have only null values, which will cause numpy to raise an error\n",
    "    # We just return a placeholder case, so that we can filter it out later\n",
    "    if not vectors.any():\n",
    "        return vectors\n",
    "    # Then we use the numpy mean method to average all those word vectors into a single vector \n",
    "    # (which is how Document.vector is created in spaCy)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create the vectorized version of our dataset\n",
    "# Each element in this list will be a single vector, representing the mean of the word vectors in a single document\n",
    "# but ONLY for the \"content\" words in that document\n",
    "doc_vecs = [vectorize_without_stops(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function does some math to find the cosine similarity \n",
    "# Based on the second answer provided here: https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists\n",
    "def cosine_sim(vec1, vec2):\n",
    "    # Each argument should be a document vector (mean of token vectors)\n",
    "    # This just returns 0 for the similarity score if one or the other of the vectors is null\n",
    "    if not vec1.any() or not vec2.any():\n",
    "        return 0\n",
    "    # This is the cosine similarity formula: the inner product of two vectors divided by the product of their norms\n",
    "    return np.inner(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use this code to create a similarity score for every document in our collection with every other document, but that's a fairly intensive computation, since our dataset contains about 8,000 documents.\n",
    "\n",
    "For illustration, let's pick one tweet and find other tweets similar to it. We can use our original dataset to include tweet metadata in our analysis.\n",
    "\n",
    "By converting our dataset of tweets into a pandas `DataFrame`, we can leverage its fast indexing and sorting methods to isolate particular documents based on their metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.DataFrame.from_records(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find Elizabeth Warren's most popular tweet.\n",
    "\n",
    "First we filter out all tweets where the name on the acount does not contain \"Warren.\"\n",
    "\n",
    "Then we sort on the `retweet_count` column to find her most popular tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>Millions may now lose their jobs. And Trump wa...</td>\n",
       "      <td>26512</td>\n",
       "      <td>Sun Mar 22 16:35:24 +0000 2020</td>\n",
       "      <td>Elizabeth Warren</td>\n",
       "      <td>SenWarren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5596</th>\n",
       "      <td>You are threatening to commit war crimes. We a...</td>\n",
       "      <td>23778</td>\n",
       "      <td>Sun Jan 05 03:35:11 +0000 2020</td>\n",
       "      <td>Elizabeth Warren</td>\n",
       "      <td>ewarren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6897</th>\n",
       "      <td>Trump told states they were on their own to pu...</td>\n",
       "      <td>23542</td>\n",
       "      <td>Tue Mar 31 15:04:44 +0000 2020</td>\n",
       "      <td>Elizabeth Warren</td>\n",
       "      <td>SenWarren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>My oldest brother, Don Reed, died from coronav...</td>\n",
       "      <td>18842</td>\n",
       "      <td>Thu Apr 23 14:39:30 +0000 2020</td>\n",
       "      <td>Elizabeth Warren</td>\n",
       "      <td>ewarren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>Russia is interfering in our election again to...</td>\n",
       "      <td>16764</td>\n",
       "      <td>Thu Feb 20 23:51:41 +0000 2020</td>\n",
       "      <td>Elizabeth Warren</td>\n",
       "      <td>ewarren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>I'll be joining @Lawrence on @MSNBC shortly to...</td>\n",
       "      <td>172</td>\n",
       "      <td>Thu Feb 06 03:26:25 +0000 2020</td>\n",
       "      <td>Elizabeth Warren</td>\n",
       "      <td>ewarren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4210</th>\n",
       "      <td>Together, we imagined a country where everyone...</td>\n",
       "      <td>169</td>\n",
       "      <td>Tue Jan 07 00:26:26 +0000 2020</td>\n",
       "      <td>Elizabeth Warren</td>\n",
       "      <td>ewarren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>It was so great to meet Vivian and Riley, two ...</td>\n",
       "      <td>139</td>\n",
       "      <td>Sat Feb 08 00:05:43 +0000 2020</td>\n",
       "      <td>Elizabeth Warren</td>\n",
       "      <td>ewarren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>With my #WealthTax, we can achieve #UniversalC...</td>\n",
       "      <td>115</td>\n",
       "      <td>Wed Jan 08 17:12:52 +0000 2020</td>\n",
       "      <td>Elizabeth Warren</td>\n",
       "      <td>ewarren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5643</th>\n",
       "      <td>Congratulations @UnitedWayofCM on your centenn...</td>\n",
       "      <td>57</td>\n",
       "      <td>Fri Jan 24 03:44:45 +0000 2020</td>\n",
       "      <td>Elizabeth Warren</td>\n",
       "      <td>SenWarren</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              full_text  retweet_count  \\\n",
       "1358  Millions may now lose their jobs. And Trump wa...          26512   \n",
       "5596  You are threatening to commit war crimes. We a...          23778   \n",
       "6897  Trump told states they were on their own to pu...          23542   \n",
       "3877  My oldest brother, Don Reed, died from coronav...          18842   \n",
       "2373  Russia is interfering in our election again to...          16764   \n",
       "...                                                 ...            ...   \n",
       "5204  I'll be joining @Lawrence on @MSNBC shortly to...            172   \n",
       "4210  Together, we imagined a country where everyone...            169   \n",
       "2972  It was so great to meet Vivian and Riley, two ...            139   \n",
       "2427  With my #WealthTax, we can achieve #UniversalC...            115   \n",
       "5643  Congratulations @UnitedWayofCM on your centenn...             57   \n",
       "\n",
       "                          created_at              name screen_name  \n",
       "1358  Sun Mar 22 16:35:24 +0000 2020  Elizabeth Warren   SenWarren  \n",
       "5596  Sun Jan 05 03:35:11 +0000 2020  Elizabeth Warren     ewarren  \n",
       "6897  Tue Mar 31 15:04:44 +0000 2020  Elizabeth Warren   SenWarren  \n",
       "3877  Thu Apr 23 14:39:30 +0000 2020  Elizabeth Warren     ewarren  \n",
       "2373  Thu Feb 20 23:51:41 +0000 2020  Elizabeth Warren     ewarren  \n",
       "...                              ...               ...         ...  \n",
       "5204  Thu Feb 06 03:26:25 +0000 2020  Elizabeth Warren     ewarren  \n",
       "4210  Tue Jan 07 00:26:26 +0000 2020  Elizabeth Warren     ewarren  \n",
       "2972  Sat Feb 08 00:05:43 +0000 2020  Elizabeth Warren     ewarren  \n",
       "2427  Wed Jan 08 17:12:52 +0000 2020  Elizabeth Warren     ewarren  \n",
       "5643  Fri Jan 24 03:44:45 +0000 2020  Elizabeth Warren   SenWarren  \n",
       "\n",
       "[251 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warren_df = tweet_df.loc[tweet_df['name'].str.contains('Warren')]\n",
    "warren_df.sort_values(by='retweet_count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because slicing and sorting a `DataFrame` don't change its index, we can use that to find the corresponding document in our collection for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1358]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "warren_tweet_index = 1358"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a function that accepts a list of document vectors and the index of one document in the collection to use as the basis for comparison. And our function should return a list giving the similarity score for each document as compared with the target document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(vectors, idx_of_target): \n",
    "    sim_scores = Counter()          # We'll again use a Counter to keep track of the scores, so that we can easily find the top scores\n",
    "    target_vec = vectors[idx_of_target]    # Assumes that our target is in the collection\n",
    "    for i, vec in enumerate(vectors): # Enumerate lets us keep track of the index of each vector in the collection\n",
    "        score = cosine_sim(target_vec, vec)   # Compute the cosine similarity\n",
    "        sim_scores[i] = score\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = compute_scores(doc_vecs, warren_tweet_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scores` object by itself doesn't tell us much, since it references each document only by its index. But it's a `Counter` object, so we can easily find the top N scores. \n",
    "\n",
    "And then we can use those entries to slice our `DataFrame` of tweets to see the tweet text and associated metadata.\n",
    "\n",
    "We have to write `score[0]` in the `DataFrame.loc[]` expression because `Counter.most_common` returns a Python tuple, the first element of which is the key -- in this case, the document index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>Millions may now lose their jobs. And Trump wa...</td>\n",
       "      <td>26512</td>\n",
       "      <td>Sun Mar 22 16:35:24 +0000 2020</td>\n",
       "      <td>Elizabeth Warren</td>\n",
       "      <td>SenWarren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>In the midst of an unprecedented national cris...</td>\n",
       "      <td>883</td>\n",
       "      <td>Sun Mar 22 19:23:48 +0000 2020</td>\n",
       "      <td>Senator Patty Murray</td>\n",
       "      <td>PattyMurray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4739</th>\n",
       "      <td>Big corporations are spending billions on stoc...</td>\n",
       "      <td>19</td>\n",
       "      <td>Thu Jan 02 20:23:35 +0000 2020</td>\n",
       "      <td>Sen. Tammy Baldwin</td>\n",
       "      <td>SenatorBaldwin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>If taxpayers are being asked to give corporati...</td>\n",
       "      <td>846</td>\n",
       "      <td>Mon Mar 23 19:18:31 +0000 2020</td>\n",
       "      <td>Mark Warner</td>\n",
       "      <td>MarkWarner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Absolutely unacceptable that Trump is prioriti...</td>\n",
       "      <td>6506</td>\n",
       "      <td>Mon May 04 01:51:17 +0000 2020</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>SenKamalaHarris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>The #PaycheckProtectionProgram has already iss...</td>\n",
       "      <td>3765</td>\n",
       "      <td>Fri Apr 17 17:11:21 +0000 2020</td>\n",
       "      <td>Rob Portman</td>\n",
       "      <td>senrobportman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5079</th>\n",
       "      <td>So when giant private equity firms don’t like ...</td>\n",
       "      <td>11</td>\n",
       "      <td>Wed Jan 08 20:48:23 +0000 2020</td>\n",
       "      <td>Senator Tina Smith</td>\n",
       "      <td>SenTinaSmith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>While millions of workers are at home without ...</td>\n",
       "      <td>14</td>\n",
       "      <td>Tue May 05 00:56:00 +0000 2020</td>\n",
       "      <td>Ed Markey</td>\n",
       "      <td>EdMarkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>\"If companies go broke, so do the jobs for the...</td>\n",
       "      <td>17</td>\n",
       "      <td>Tue Mar 24 18:14:25 +0000 2020</td>\n",
       "      <td>Senator Roger Wicker</td>\n",
       "      <td>SenatorWicker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7444</th>\n",
       "      <td>Any stimulus must support workers’ payroll and...</td>\n",
       "      <td>2393</td>\n",
       "      <td>Mon Mar 23 17:26:27 +0000 2020</td>\n",
       "      <td>Elizabeth Warren</td>\n",
       "      <td>ewarren</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              full_text  retweet_count  \\\n",
       "1358  Millions may now lose their jobs. And Trump wa...          26512   \n",
       "882   In the midst of an unprecedented national cris...            883   \n",
       "4739  Big corporations are spending billions on stoc...             19   \n",
       "1828  If taxpayers are being asked to give corporati...            846   \n",
       "351   Absolutely unacceptable that Trump is prioriti...           6506   \n",
       "3498  The #PaycheckProtectionProgram has already iss...           3765   \n",
       "5079  So when giant private equity firms don’t like ...             11   \n",
       "386   While millions of workers are at home without ...             14   \n",
       "4039  \"If companies go broke, so do the jobs for the...             17   \n",
       "7444  Any stimulus must support workers’ payroll and...           2393   \n",
       "\n",
       "                          created_at                  name      screen_name  \n",
       "1358  Sun Mar 22 16:35:24 +0000 2020      Elizabeth Warren        SenWarren  \n",
       "882   Sun Mar 22 19:23:48 +0000 2020  Senator Patty Murray      PattyMurray  \n",
       "4739  Thu Jan 02 20:23:35 +0000 2020    Sen. Tammy Baldwin   SenatorBaldwin  \n",
       "1828  Mon Mar 23 19:18:31 +0000 2020           Mark Warner       MarkWarner  \n",
       "351   Mon May 04 01:51:17 +0000 2020         Kamala Harris  SenKamalaHarris  \n",
       "3498  Fri Apr 17 17:11:21 +0000 2020           Rob Portman    senrobportman  \n",
       "5079  Wed Jan 08 20:48:23 +0000 2020    Senator Tina Smith     SenTinaSmith  \n",
       "386   Tue May 05 00:56:00 +0000 2020             Ed Markey         EdMarkey  \n",
       "4039  Tue Mar 24 18:14:25 +0000 2020  Senator Roger Wicker    SenatorWicker  \n",
       "7444  Mon Mar 23 17:26:27 +0000 2020      Elizabeth Warren          ewarren  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.loc[[score[0] for score in scores.most_common(10)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be easier to see the full text if we just inspect the `.values` attribute of that column.\n",
    "\n",
    "We can see that our similarity scoring did a pretty good job of identifying as most similar tweets about big corporations and billionaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Millions may now lose their jobs. And Trump wants our response to be a half-trillion dollar slush fund to boost favored companies and corporate executives – while they continue to pull down huge paychecks and fire their workers. Here’s what I know and how we stop it:',\n",
       "       'In the midst of an unprecedented national crisis, Republicans can’t seriously expect us to tell people who are suffering that we shortchanged hospitals, students, workers, &amp; small biz but gave big corporations hundreds of billions of dollars in a secretive slush fund.',\n",
       "       'Big corporations are spending billions on stock buybacks to reward wealthy shareholders, while workers are getting pink slips.\\n\\nWe need my #RewardWork Act to rein in corporate stock buybacks &amp; give workers a voice in how their company’s profits are spent https://t.co/MECn84WBSq',\n",
       "       'If taxpayers are being asked to give corporations a multi-billion-dollar lifeline, there need to be some strings attached.\\n \\nWe’ve got to make sure that money is spent keeping workers on the payroll, not on stock buybacks or executive compensation.',\n",
       "       'Absolutely unacceptable that Trump is prioritizing bailing out Big Oil companies while dragging his feet to support millions of unemployed people, workers, small businesses, and state and local governments. The American people are tired of this.',\n",
       "       'The #PaycheckProtectionProgram has already issued 1M+ loans to small businesses, saving millions of jobs nationwide.\\n \\nBut yesterday the program ran out of money. And Democrats have blocked a measure to replenish it.\\n \\nLet’s stop playing politics &amp; fund this vital lifeline now. https://t.co/OIYDnYeFg2',\n",
       "       'So when giant private equity firms don’t like legislation we’re working on to end surprise medical billing, they fund a dark money group and blitz the airwaves with MILLIONS in misleading ads. Got it.\\n\\nAt least have the guts to say who you are.  \\nhttps://t.co/uGVKwa9dWs',\n",
       "       'While millions of workers are at home without a paycheck, Trump has chosen to put more money into the pockets of his fossil fuel friends. I stand with my Senate colleagues and demand these funds be delivered to small businesses who urgently need this aid.\\nhttps://t.co/aOk59wr5GK',\n",
       "       '\"If companies go broke, so do the jobs for the employees... The reason to help companies hurt by this government-mandated national economic shutdown is so workers will have good jobs with good benefits to return to.\" - @WSJopinion https://t.co/evfzmDppx8',\n",
       "       'Any stimulus must support workers’ payroll and benefits—not enrich wealthy executives of giant corporations. If we want to weather this crisis, we need to build the economy from the grassroots up. https://t.co/Ob6JkRUIcU'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.loc[[score[0] for score in scores.most_common(10)]]['full_text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've taken our first steps toward building a document classifier! The initial results look pretty good, though as with any use of NLP, it's worth doing more exploration in order to evaluate the accuracy and robustness of the method and possibly to fine tune it. \n",
    "\n",
    "If you're interested in document classification, there are other approaches that don't use word vectors/embeddings. **Topic modeling** is among the most widely used. \n",
    "\n",
    "In recent years, **deep neural networks** have garnered a lot of attention in NLP, including for text classification. spaCy includes a `TextCategorizer` component that can be used to train a network for text classification. Unlike topic modeling and other statistical approaches, such networks usually require a labeled training dataset. \n",
    "\n",
    "See the [spaCy docs](https://spacy.io/usage/training#textcat) for more info."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
